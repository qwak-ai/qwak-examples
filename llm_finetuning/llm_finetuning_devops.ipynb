{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Qwen 1.5 Model and Logging to a Model Registry\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning a small-scale Qwen model (`Qwen/Qwen1.5-0.5B-Chat`) on a public instruction-based dataset. We will use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to make the process memory-efficient.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Setup**: Install required libraries and import necessary modules.\n",
    "2.  **Configuration**: Define all parameters for the model, dataset, and training.\n",
    "3.  **Data Preparation**: Load and prepare the dataset for instruction fine-tuning.\n",
    "4.  **Model Loading and Fine-Tuning**: Load the pre-trained model and tokenizer, and then fine-tune it using `trl`'s `SFTTrainer`.\n",
    "5.  **Evaluation**: Compare the performance of the base model with the fine-tuned model.\n",
    "6.  **Model Logging**: Log the fine-tuned model and its metrics to a model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we'll install the necessary Python libraries and import all the required modules for the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import frogml # Assuming frogml is the library for your JFrog integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define all our configurations in one place. This makes the notebook cleaner and easier to modify for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer configuration\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "new_model_adapter = \"qwen-0.5b-devops-adapter\"\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name = \"Szaid3680/Devops\"\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=1,\n",
    "    fp16=False, # Ensure this is False for CPU/MPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We will load the `Szaid3680/Devops` dataset, split it into training and evaluation sets, and define a formatting function for instruction-based fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the training dataset:\n",
      "{'Response': 'yes, as thekubectl and kptsays, the first step in getting prepared to install cluster is installinggcloudthat is CLI that manages authentication, local configuration, developer workflow, interactions withGoogle Cloud APIs.\\nWithout is you simply cant work with objects(in your case you need to enablekpt anthoscli beta) and perform tasks likecreating a Compute Engine VM instance, managing a Google Kubernetes\\nEngine cluster, and deploying an App Engine application, either from\\nthe command line or in scripts and other automations..', 'Instruction': 'I am trying to use KubeFlow on GCP and I am following thiscodelab, but \"click-to-deploy\" is no longer supported so I followed the documentation of \"kubectl and kpt\". However, I keep getting this \"You cannot perform this action because the Cloud SDK component manager is disabled for this installation.\" error and none of the solutions I found worked. I have 2 other friends told me they tried to make KubeFlow work since last year, it never worked, but I did see people post question about KubeFlow on Stackoverflow still, so I want to askif it is still working, if so, where can I find a decent guide to follow?Thanks!', 'Prompt': 'Is KubeFlow still supported on GCP?'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# For a quick demo, we'll use a small subset of the data\n",
    "train_dataset = train_dataset.select(range(2))\n",
    "eval_dataset = eval_dataset.select(range(2))\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Formats the dataset examples into a structured prompt.\"\"\"\n",
    "    instruction = example.get('Instruction', '')\n",
    "    inp = example.get('Prompt', '')\n",
    "    response = example.get('Response', '')\n",
    "    \n",
    "    full_prompt = f\"<s>[INST] {instruction}\\n{inp} [/INST] {response} </s>\"\n",
    "    return full_prompt\n",
    "\n",
    "# Let's look at a sample from the training set\n",
    "print(\"Sample from the training dataset:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Fine-Tuning\n",
    "\n",
    "Now, we'll load the base model and tokenizer. Then, we will apply the LoRA configuration and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 2/2 [00:00<00:00, 287.99 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 782.81 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 2/2 [00:00<00:00, 848.19 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2/2 [00:00<00:00, 109.06 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2/2 [00:00<00:00, 602.41 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 625.18 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 1086.33 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 1113.58 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2/2 [00:00<00:00, 422.49 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2/2 [00:00<00:00, 875.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.376900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\" # Use CPU for local demo\n",
    ")\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_instruction,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"--- Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model and compare its response to the base model's response for a sample DevOps-related prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "{'eval_loss': 3.707406520843506, 'eval_runtime': 3.3473, 'eval_samples_per_second': 0.598, 'eval_steps_per_second': 0.299}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model adapter\n",
    "trainer.model.save_pretrained(new_model_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- FINE-TUNED MODEL RESPONSE -------------------\n",
      "To expose a deployment in Kubernetes using a service, you can follow these steps:\n",
      "\n",
      "  1. Create a Kubernetes deployment for the service by running `kubectl create deployment deployment_name` in the terminal.\n",
      "  2. Define the service and its labels, such as `apiVersion: apps/v1`, `kind: Deployment`, and `metadata如下:\n",
      "```\n",
      "        type: Deployment\n",
      "      spec:\n",
      "        selector:\n",
      "          matchLabels:\n",
      "            app: my-app\n",
      "          namespace: default\n",
      "        template:\n",
      "          metadata:\n",
      "            labels:\n",
      "              app: my-app\n",
      "          spec:\n",
      "            containers:\n",
      "              - name: my-app\n",
      "                image: my-app:latest\n",
      "                ports:\n",
      "                  - containerPort: 80\n",
      "```\n",
      "\n",
      "In this example, we define a `Deployment` with an ID of `deployment_name`. We also specify that the deployment should be applied to the `default` namespace.\n",
      "\n",
      "  3. Run `kubectl apply -f deployment.yaml` in the terminal to create the deployment.\n",
      "  4. Run `kubectl get deployments` in the terminal to list all the existing deployment definitions.\n",
      "  5. If you want to change the labels or spec of a specific deployment, you can use the `kubectl labelapply` command to add or\n",
      "\n",
      "------------------- BASE MODEL RESPONSE -------------------\n",
      "To expose a deployment in Kubernetes, you can use the `kubectl` command-line tool or the `kubectl service` API.\n",
      "Here is an example of how to use `kubectl service` to expose a deployment in Kubernetes:\n",
      "```\n",
      "kubectl service deployment my-deployment\n",
      "```\n",
      "\n",
      "This will create a new service called \"my-deployment\" with the specified label and port number for your deployment. You can then use this service to deploy any code that requires access to the same network as the application being deployed.\n",
      "Alternatively, you can use the `kubectl command-line tool` to expose a deployment in Kubernetes by creating a new YAML file containing the desired configuration and then running the following command:\n",
      "```\n",
      "kubectl apply -f deployment.yaml\n",
      "```\n",
      "\n",
      "This will create a new deployment with the specified name and labels.\n",
      "Both methods require that you have already set up Kubernetes in your environment and have the necessary permissions to manage resources. It's also important to make sure that the deployment you're exposing is running on a separate machine than the container it is running in, and that the application is not conflicting with any other services or applications running on the cluster.\n"
     ]
    }
   ],
   "source": [
    "# Merge the LoRA adapter with the base model for easy inference\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, new_model_adapter)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Define a prompt for evaluation\n",
    "prompt = \"How do I expose a deployment in Kubernetes using a service?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful DevOps assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "print(\"------------------- FINE-TUNED MODEL RESPONSE -------------------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cpu\")\n",
    "# Store the length of the input prompt tokens\n",
    "input_ids_len = model_inputs['input_ids'].shape[1]\n",
    "generated_ids = finetuned_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "\n",
    "# We keep all batch items (:) and slice each one from the end of the input length onwards.\n",
    "response_only_ids = generated_ids[:, input_ids_len:]\n",
    "response_finetuned = tokenizer.decode(response_only_ids[0], skip_special_tokens=True)\n",
    "print(response_finetuned)\n",
    "\n",
    "# Generate response from the original base model for comparison\n",
    "print(\"\\n------------------- BASE MODEL RESPONSE -------------------\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "generated_ids_base = original_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_only_ids_base = generated_ids_base[:, input_ids_len:]\n",
    "response_base = tokenizer.decode(response_only_ids_base[0], skip_special_tokens=True)\n",
    "print(response_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Logging\n",
    "\n",
    "Finally, we log our fine-tuned model, its tokenizer, and the evaluation metrics to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:frogml.sdk.model_version.utils.model_log_config:No version provided; using current datetime as the version\n",
      "INFO:HuggingfaceModelVersionManager:Logging model finetuned_llm to llms\n",
      "INFO:JmlCustomerClient:Customer exists in JML.\n",
      "INFO:JmlCustomerClient:Getting project key for repository llms\n",
      "INFO:frogml.sdk.model_version.utils.files_tools:Code directory, predict file and dependencies are provided. Setup template files for model_name finetuned_llm\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/tokenizer_config.json: 100%|██████████| 970/970 [00:00<00:00, 4.20MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/special_tokens_map.json: 100%|██████████| 250/250 [00:00<00:00, 280kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 1.63MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 208GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/config.json: 100%|██████████| 1.23k/1.23k [00:00<00:00, 3.34kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 46.4GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/config.json: 100%|██████████| 1.23k/1.23k [00:00<00:00, 1.38kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/chat_template.jinja: 100%|██████████| 328/328 [00:00<00:00, 9.90MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/generation_config.json: 100%|██████████| 205/205 [00:00<00:00, 311B/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 77.6GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/model.safetensors: 100%|██████████| 1.86G/1.86G [04:34<00:00, 6.76MB/s]\n",
      "/Users/haha/Projects/qwak-examples/llm_finetuning/main/conda.yaml: 100%|██████████| 284/284 [00:00<00:00, 5.67MB/s]\n",
      "/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/code.zip: 100%|██████████| 2.71k/2.71k [00:00<00:00, 4.19kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-20 18:00:19,084 - INFO - frogml.storage.logging._log_config.frog_ml.__upload_model:533 - Model: \"finetuned_llm\", version: \"2025-08-20-14-55-37-138\" has been uploaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Logged Successfully ---\n"
     ]
    }
   ],
   "source": [
    "# REPLACE WITH YOUR OWN FILESYSTEM BASE PATH WHERE THE PROJECTS RESIDE\n",
    "base_projects_directory = \"/root/jfrog\"  # change it to your own projects path where the examples repo was cloned to\n",
    "\n",
    "try:\n",
    "    import frogml\n",
    "\n",
    "    frogml.huggingface.log_model(   \n",
    "    model= finetuned_model,\n",
    "        tokenizer= tokenizer,\n",
    "        repository=\"llm\",    # The JFrog repository to upload the model to.\n",
    "        model_name=\"finetuned_qwen\",     # The uploaded model name\n",
    "        version=\"\",     # Optional. The uploaded model version\n",
    "        parameters={\"finetuning-dataset\": dataset_name},\n",
    "        code_dir=f\"{base_projects_directory}/qwak-examples/llm_finetuning/code_dir\",\n",
    "        dependencies=[f\"{base_projects_directory}/qwak-examples/llm_finetuning/main/conda.yaml\"],\n",
    "        metrics = metrics,\n",
    "        predict_file=f\"{base_projects_directory}/qwak-examples/llm_finetuning/code_dir/predict.py\"\n",
    "        )\n",
    "    print(\"--- Model Logged Successfully ---\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model logging: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
