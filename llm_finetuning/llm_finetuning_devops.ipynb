{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Qwen 1.5 Model and Logging to a Model Registry\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning a small-scale Qwen model (`Qwen/Qwen1.5-0.5B-Chat`) on a public instruction-based dataset. We will use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to make the process memory-efficient.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Setup**: Install required libraries and import necessary modules.\n",
    "2.  **Configuration**: Define all parameters for the model, dataset, and training.\n",
    "3.  **Data Preparation**: Load and prepare the dataset for instruction fine-tuning.\n",
    "4.  **Model Loading and Fine-Tuning**: Load the pre-trained model and tokenizer, and then fine-tune it using `trl`'s `SFTTrainer`.\n",
    "5.  **Evaluation**: Compare the performance of the base model with the fine-tuned model.\n",
    "6.  **Model Logging**: Log the fine-tuned model and its metrics to a model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we'll install the necessary Python libraries and import all the required modules for the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import frogml # Assuming frogml is the library for your JFrog integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define all our configurations in one place. This makes the notebook cleaner and easier to modify for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer configuration\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "new_model_adapter = \"qwen-0.5b-devops-adapter\"\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name = \"Szaid3680/Devops\"\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    fp16=False, # Ensure this is False for CPU/MPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We will load the `Szaid3680/Devops` dataset, split it into training and evaluation sets, and define a formatting function for instruction-based fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the training dataset:\n",
      "{'Response': \"\\n\\n\\n\\n\\n\\n\\n\\n            0\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nI strongly recommend you to keep track of user's data on your server. \\nThat way, even if the user deletes the app, the data will still be available after retrieving it from the server. Also, this will make you able to sync the same data between different platforms, not only devices.\\nAfter you retrieve the data, you might want to store it in NSUserDefaults and updated when needed.\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\nImprove this answer\\n\\n\\n\\n                        Follow\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            answered May 22, 2014 at 6:46\\n\\n\\n\\n\\n\\n\\nSebydddSebyddd\\n\\n4,30522 gold badges3939 silver badges4343 bronze badges\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdd a comment\\n\\xa0|\\xa0\\n\\n\\n\\n\\n\", 'Instruction': '\\nI have implemented in-app-purchases like Mission packs or \"full version\" before. However, I am now looking into selling in-game credits. \\nWhat are some ways of keeping track of spent and total credits, even after removing and reinstalling the app \\nIs it common to sync these totals between the different iOS devices of a single user? Or should a user re-buy credits on different devices? \\nShould I have a user register with my server and track the credit on there? \\n', 'Prompt': 'Backing up in app purchases like gold or credits'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# For a quick demo, we'll use a small subset of the data\n",
    "train_dataset = train_dataset.select(range(2))\n",
    "eval_dataset = eval_dataset.select(range(2))\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Formats the dataset examples into a structured prompt.\"\"\"\n",
    "    instruction = example.get('Instruction', '')\n",
    "    inp = example.get('Prompt', '')\n",
    "    response = example.get('Response', '')\n",
    "    \n",
    "    full_prompt = f\"<s>[INST] {instruction}\\n{inp} [/INST] {response} </s>\"\n",
    "    return full_prompt\n",
    "\n",
    "# Let's look at a sample from the training set\n",
    "print(\"Sample from the training dataset:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Fine-Tuning\n",
    "\n",
    "Now, we'll load the base model and tokenizer. Then, we will apply the LoRA configuration and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 2/2 [00:00<00:00, 124.25 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 854.76 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 2/2 [00:00<00:00, 945.09 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2/2 [00:00<00:00, 135.71 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2/2 [00:00<00:00, 609.37 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 653.83 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 1065.49 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 899.39 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2/2 [00:00<00:00, 406.42 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2/2 [00:00<00:00, 463.48 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:48, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\" # Use CPU for local demo\n",
    ")\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_instruction,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"--- Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model and compare its response to the base model's response for a sample DevOps-related prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "{'eval_loss': 5.667745113372803, 'eval_runtime': 3.329, 'eval_samples_per_second': 0.601, 'eval_steps_per_second': 0.3}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model adapter\n",
    "trainer.model.save_pretrained(new_model_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "------------------- FINE-TUNED MODEL RESPONSE -------------------\n",
      "system\n",
      "You are a helpful DevOps assistant.\n",
      "user\n",
      "How do I expose a deployment in Kubernetes using a service?\n",
      "assistant\n",
      "To expose a deployment in Kubernetes using a service, you can follow these steps:\n",
      "\n",
      "1. Define the desired service: Start by defining the service that you want to expose. This should include the name of the service, the API group that it belongs to (if applicable), and any other details that you need to specify.\n",
      "\n",
      "2. Create the Kubernetes API resource: Use the Kubernetes CLI to create an API resource for your service. This will involve creating a secret with the name \"service_name\" and setting its value to the desired service name.\n",
      "\n",
      "3. Add the app definition: In the API resource, add the app definition for your deployment. This includes specifying the desired configuration (e.g., resources, API version, etc.) and any additional settings that you need to apply to your instance.\n",
      "\n",
      "4. Build the Kubernetes manifest: Use the Kubernetes command line or the built-in tooling to build the Kubernetes manifest for your service. This will include creating the service selector and any other Kubernetes-related metadata.\n",
      "\n",
      "5. Apply the change: Once the manifest is built, apply it to the Kubernetes cluster using the Kubernetes operator or the built-in job runner.\n",
      "\n",
      "6. Verify the deployment: After applying the change, verify that the deployment is running as expected by running the desired operations on it (e\n",
      "\n",
      "------------------- BASE MODEL RESPONSE -------------------\n",
      "system\n",
      "You are a helpful DevOps assistant.\n",
      "user\n",
      "How do I expose a deployment in Kubernetes using a service?\n",
      "assistant\n",
      "To expose a deployment in Kubernetes using a service, you can follow these steps:\n",
      "\n",
      "  1. Identify the service that needs to be exposed: Determine which Kubernetes service your application needs to interact with.\n",
      "  2. Create an API Gateway: A service API Gateway is a way for your application to expose its service as a RESTful API.\n",
      "  3. Set up a Service account: A Service account is a set of credentials needed to access the service API. You'll need to create a new Service account and assign it to the application.\n",
      "  4. Create a Kubernetes secret: You'll also need to create a Kubernetes secret to store the secret key used to authenticate with the service API.\n",
      "  5. Add the Service account and secret to the Service discovery zone: In the Kube config file, add the following configuration to the Service Discovery Zone:\n",
      "```\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: my-service\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: my-app\n",
      "  ports:\n",
      "    - port: 80\n",
      "```\n",
      "\n",
      "\n",
      "  6. Start the service: Once the service is created, you can start it using the `kubectl apply` command.\n",
      "\n",
      "Note: The exact steps may vary depending\n"
     ]
    }
   ],
   "source": [
    "# Merge the LoRA adapter with the base model for easy inference\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, new_model_adapter)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Define a prompt for evaluation\n",
    "prompt = \"How do I expose a deployment in Kubernetes using a service?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful DevOps assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "print(\"------------------- FINE-TUNED MODEL RESPONSE -------------------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cpu\")\n",
    "generated_ids = finetuned_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_finetuned = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(response_finetuned)\n",
    "\n",
    "# Generate response from the original base model for comparison\n",
    "print(\"\\n------------------- BASE MODEL RESPONSE -------------------\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "generated_ids_base = original_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_base = tokenizer.decode(generated_ids_base[0], skip_special_tokens=True)\n",
    "print(response_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Logging\n",
    "\n",
    "Finally, we log our fine-tuned model, its tokenizer, and the evaluation metrics to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:frogml.sdk.model_version.utils.model_log_config:No version provided; using current datetime as the version\n",
      "INFO:HuggingfaceModelVersionManager:Logging model finetuned_qwen to llms\n",
      "INFO:JmlCustomerClient:Customer exists in JML.\n",
      "INFO:JmlCustomerClient:Getting project key for repository llms\n",
      "INFO:frogml.sdk.model_version.utils.files_tools:Code directory, predict file and dependencies are provided. Setup template files for model_name finetuned_qwen\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 1.78MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/tokenizer_config.json: 100%|██████████| 1.33k/1.33k [00:00<00:00, 49.7MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/config.json: 100%|██████████| 729/729 [00:00<00:00, 24.3MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/special_tokens_map.json: 100%|██████████| 250/250 [00:00<00:00, 7.38MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 460GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/generation_config.json: 100%|██████████| 205/205 [00:00<00:00, 3.31MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 9.89GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 55.7GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/finetuned_qwen.pretrained_model/model.safetensors: 100%|██████████| 1.86G/1.86G [00:00<00:00, 20.6TB/s]\n",
      "/Users/haha/Projects/qwak-examples/llm_finetuning/main/conda.yaml: 100%|██████████| 284/284 [00:00<00:00, 4.16MB/s]\n",
      "/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmpm7sazn05/code.zip: 100%|██████████| 2.60k/2.60k [00:00<00:00, 2.78kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 12:45:51,638 - INFO - frogml.storage.logging._log_config.frog_ml.__upload_model:533 - Model: \"finetuned_qwen\", version: \"2025-08-05-09-45-26-787\" has been uploaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Logged Successfully ---\n"
     ]
    }
   ],
   "source": [
    "# REPLACE WITH YOUR OWN FILESYSTEM BASE PATH WHERE THE PROJECTS RESIDE\n",
    "base_projects_directory = \"your_projects_path\"\n",
    "\n",
    "try:\n",
    "    import frogml\n",
    "\n",
    "    frogml.huggingface.log_model(   \n",
    "    model= finetuned_model,\n",
    "        tokenizer= tokenizer,\n",
    "        repository=\"llms\",    # The JFrog repository to upload the model to.\n",
    "        model_name=\"finetuned_qwen\",     # The uploaded model name\n",
    "        version=\"\",     # Optional. The uploaded model version\n",
    "        parameters={\"finetuning-dataset\": dataset_name},\n",
    "        code_dir=f\"{base_projects_directory}/qwak-examples/llm_finetuning/code_dir\",\n",
    "        dependencies=[f\"{base_projects_directory}/qwak-examples/llm_finetuning/main/conda.yaml\"],\n",
    "        metrics = metrics,\n",
    "        predict_file=f\"{base_projects_directory}/qwak-examples/llm_finetuning/code_dir/predict.py\"\n",
    "        )\n",
    "    print(\"--- Model Logged Successfully ---\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model logging: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
