{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Qwen 1.5 Model and Logging to a Model Registry\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning a small-scale Qwen model (`Qwen/Qwen1.5-0.5B-Chat`) on a public instruction-based dataset. We will use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to make the process memory-efficient.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Setup**: Install required libraries and import necessary modules.\n",
    "2.  **Configuration**: Define all parameters for the model, dataset, and training.\n",
    "3.  **Data Preparation**: Load and prepare the dataset for instruction fine-tuning.\n",
    "4.  **Model Loading and Fine-Tuning**: Load the pre-trained model and tokenizer, and then fine-tune it using `trl`'s `SFTTrainer`.\n",
    "5.  **Evaluation**: Compare the performance of the base model with the fine-tuned model.\n",
    "6.  **Model Logging**: Log the fine-tuned model and its metrics to a model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we'll install the necessary Python libraries and import all the required modules for the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import frogml # Assuming frogml is the library for your JFrog integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define all our configurations in one place. This makes the notebook cleaner and easier to modify for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer configuration\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "new_model_adapter = \"qwen-0.5b-devops-adapter\"\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name = \"Szaid3680/Devops\"\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    fp16=False, # Ensure this is False for CPU/MPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We will load the `Szaid3680/Devops` dataset, split it into training and evaluation sets, and define a formatting function for instruction-based fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the training dataset:\n",
      "{'Response': 'Establishing a database connection is a pretty expensive operation. Ideally a web application should be using a connection pool, so that you create create pool of database sessions initially and they remain there for the life of the application. The app tier will ask for a connection from the pool as it needs to interact with the database.So utopia is to see an initial set of LOGON records and then no LOGOFF records until your shut the application down.ShareFollowansweredMar 11, 2022 at 8:03Connor McDonaldConnor McDonald10.9k11 gold badge1212 silver badges1919 bronze badgesAdd a comment|', 'Instruction': 'I have a web application built by ASP.NET Web API and the database is Oracle.When I published the site on the IIS and run it, I recognized the following:I found many records in the viewDBA_AUDIT_SESSIONand that\\'s recordsLOGOFF/LOGONin the order.After that, I let the site open for a while on a tab in the Chrome Browser without any interaction from me and I found many recordsLOGOFF, then return to the tab and open a page included it, and I found new records was generated on that view but with ActionLOGON.My question is:1-That\\'s normal or my application has an issue?I analyzed that, but maybe I was wrong:I think when the site run using IIS, all the old sessions will beLOGOFF, and after the application run on the browser, new sessions will be generated, and when the application sleep \"without any interaction\" the session will beLOGOFFand after return to the application and ask for a page included it, the application will ask a data from the database using an API and that connection will register as LOGON action.Another question:I check the main viewDBA_AUDIT_TRAILand I found the ActionLOGONcome withcomment_textexplain the Authentication process, and in my case, it comes with:Authenticated by: DATABASE;My question is in everyLOGONaction the port changed, why that?', 'Prompt': 'LOGON/LOGOFF in the view DBA_AUDIT_SESSION'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# For a quick demo, we'll use a small subset of the data\n",
    "train_dataset = train_dataset.select(range(2))\n",
    "eval_dataset = eval_dataset.select(range(2))\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Formats the dataset examples into a structured prompt.\"\"\"\n",
    "    instruction = example.get('Instruction', '')\n",
    "    inp = example.get('Prompt', '')\n",
    "    response = example.get('Response', '')\n",
    "    \n",
    "    full_prompt = f\"<s>[INST] {instruction}\\n{inp} [/INST] {response} </s>\"\n",
    "    return full_prompt\n",
    "\n",
    "# Let's look at a sample from the training set\n",
    "print(\"Sample from the training dataset:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Fine-Tuning\n",
    "\n",
    "Now, we'll load the base model and tokenizer. Then, we will apply the LoRA configuration and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 2/2 [00:00<00:00, 387.16 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 2/2 [00:00<00:00, 945.20 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2/2 [00:00<00:00, 84.95 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2/2 [00:00<00:00, 497.75 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 616.54 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 840.96 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2/2 [00:00<00:00, 289.59 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2/2 [00:00<00:00, 722.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:34, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\" # Use CPU for local demo\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_instruction,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"--- Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model and compare its response to the base model's response for a sample DevOps-related prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "{'eval_loss': 5.611345291137695, 'eval_runtime': 3.4177, 'eval_samples_per_second': 0.585, 'eval_steps_per_second': 0.293}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model adapter\n",
    "trainer.model.save_pretrained(new_model_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuned Model Response ---\n",
      "system\n",
      "You are a helpful DevOps assistant.\n",
      "user\n",
      "How do I expose a deployment in Kubernetes using a service?\n",
      "assistant\n",
      "To expose a deployment in Kubernetes using a service, you can follow these steps:\n",
      "\n",
      "  1. Create a Kubernetes resource group for your application and label it with the appropriate namespace.\n",
      "  2. Define the service that you want to expose. You can use the `app` label to specify the name of your app, and the container image to specify the runtime environment.\n",
      "  3. In the Kubernetes API, you can find services by their name. Find the service you created in step 2, then select the associated resource group.\n",
      "  4. Make the desired setup and apply any other settings as desired.\n",
      "\n",
      "For example, if your application needs to be accessible over a specific port (e.g., 80), you would set the service to listen on that port. If your application does not need to be accessible over a specific port, you would use the default setting of serving all requests.\n",
      "I hope this helps! Let me know if you have any questions.\n",
      "\n",
      "--- Base Model Response ---\n",
      "system\n",
      "You are a helpful DevOps assistant.\n",
      "user\n",
      "How do I expose a deployment in Kubernetes using a service?\n",
      "assistant\n",
      "To expose a deployment in Kubernetes using a service, you can use the `kubectl apply` command along with the `apiVersion: apps/v1`, `kind: Deployment`, and `metadata` options to create a new deployment with the specified name and metadata.\n",
      "Here is an example of how you might set up a deployment using `kubectl apply`:\n",
      "```\n",
      "kubectl apply -f my-deployment.yaml\n",
      "```\n",
      "\n",
      "In this example, `my-deployment.yaml` is a file that defines the properties of your deployment. You can customize the file by adding or removing fields as needed.\n",
      "Once the deployment has been created, you can manage it using the `kubectl get` command to view its details. To manage the deployment, you can use the `kubectl logs` command to view information about its status, including any errors or warnings that may be occurring.\n",
      "```perl\n",
      "kubectl logs my-deployment\n",
      "```\n",
      "\n",
      "This will show you a list of all events that occurred during the process of deploying your application. You can also use the `kubectl delete` command to delete the deployment and its associated resources.\n",
      "It's worth noting that there may be additional steps involved depending on the specific requirements of your application. For example, you may need to create a template for the deployment that specifies the\n"
     ]
    }
   ],
   "source": [
    "# Merge the LoRA adapter with the base model for easy inference\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, new_model_adapter)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Define a prompt for evaluation\n",
    "prompt = \"How do I expose a deployment in Kubernetes using a service?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful DevOps assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "print(\"------------------- FINE-TUNED MODEL RESPONSE -------------------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cpu\")\n",
    "generated_ids = finetuned_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_finetuned = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(response_finetuned)\n",
    "\n",
    "# Generate response from the original base model for comparison\n",
    "print(\"\\n------------------- BASE MODEL RESPONSE -------------------\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "generated_ids_base = original_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_base = tokenizer.decode(generated_ids_base[0], skip_special_tokens=True)\n",
    "print(response_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Logging\n",
    "\n",
    "Finally, we log our fine-tuned model, its tokenizer, and the evaluation metrics to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:HuggingfaceModelVersionManager:Logging model finetuned_qwen to llms\n",
      "INFO:JmlCustomerClient:Getting project key for repository llms\n",
      "INFO:JmlCustomerClient:Customer exists in JML.\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/tokenizer_config.json: 100%|██████████| 970/970 [00:00<00:00, 2.69kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/config.json: 100%|██████████| 684/684 [00:00<00:00, 13.5MB/s]\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 2.35MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/tokenizer_config.json: 100%|██████████| 970/970 [00:00<00:00, 1.33kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 69.4GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/special_tokens_map.json: 100%|██████████| 250/250 [00:00<00:00, 362B/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 207GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/generation_config.json: 100%|██████████| 205/205 [00:00<00:00, 2.32MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 42.2GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/chat_template.jinja: 100%|██████████| 328/328 [00:00<00:00, 2.80MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp6kwjnj6m/finetuned_qwen.pretrained_model/model.safetensors:  82%|████████▏ | 1.52G/1.86G [00:42<00:08, 41.6MB/s]   "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import frogml\n",
    "\n",
    "    frogml.huggingface.log_model(   \n",
    "    model= finetuned_model,\n",
    "        tokenizer= tokenizer,\n",
    "        repository=\"llms\",    # The JFrog repository to upload the model to.\n",
    "        model_name=\"finetuned_qwen\",     # The uploaded model name\n",
    "        version=\"v0-2.2\",     # Optional. The uploaded model version\n",
    "        properties = {\"dataset\": \"Szaid3680-Devops\"},\n",
    "        metrics = metrics\n",
    "        )\n",
    "    print(\"--- Model Logged Successfully ---\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model logging: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
