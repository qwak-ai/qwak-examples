{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1f8ccd-5592-4938-abc9-0cd3334603fa",
   "metadata": {},
   "source": [
    "## Qwak Streaming Feature Set with Streaming Aggregations, Offline, and Online Querying\n",
    "\n",
    "Welcome to the Qwak Feature Store example! In this tutorial, we'll guide you through creating a sample Streaming Data Source, defining the streaming ingestion pipeline and sample aggregations, and extracting features from both the Offline and Online store using the Qwak SDK. \n",
    "\n",
    "Guides like this one aim to provide you with a starting point by offering a straightforward framework for working with Qwak. However, we encourage you to explore our [feature store overview](https://docs-saas.qwak.com/docs/feature-store-overview) and [streaming documentation](https://docs-saas.qwak.com/docs/streaming-feature-set) for a more comprehensive explanation.\n",
    "\n",
    "Before diving in, make sure you have the Qwak SDK installed and authenticated. If you haven't done so already, follow these steps:\n",
    "\n",
    "1. [Install the Qwak SDK](https://docs-saas.qwak.com/docs/installing-the-qwak-sdk) - Ensure you have the SDK installed on your local environment.\n",
    "2. [Authenticate](https://docs-saas.qwak.com/docs/installing-the-qwak-sdk#1-via-qwak-cli) - Authenticate with a new Personal or Service Qwak API Key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcf64f-129d-4df0-8681-50ed1d0ba93e",
   "metadata": {},
   "source": [
    "## Create the Kafka Streaming Data Source\n",
    "\n",
    "In Qwak, a Data Source serves as a configuration object that specifies how to access and fetch your data. It includes metadata such as name and description, connection details to the Kafka brokers, instructions for handling the schema of the incoming events, and any relevant configuration settings specific to your Kafka environment.\n",
    "\n",
    "### Components of a Streaming Data Source:\n",
    "\n",
    "1. **Metadata**: Includes information like name, description, etc.\n",
    "2. **Topic**: The Kafka topic to subscribe and ingest from\n",
    "3. **Bootstrap Servers**: The explicit Kafka brokers to connect to in your Kafka cluster. \n",
    "4. **Deserializer**: A function to handle the schema of the incoming Kafka event. This can be done automatically using our [Generic Deserializer](https://docs-saas.qwak.com/docs/streaming-data-sources#generic-deserializer-1) or using a [Custom Deserializer](https://docs-saas.qwak.com/docs/streaming-data-sources#custom-deserializer-1)\n",
    "5. **Passthrough Configs**: Any additional Kafka related settings such as offset related configurations. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e44ba-10a0-406f-9e20-647004e37af8",
   "metadata": {},
   "source": [
    "In the following example, we'll demonstrate connecting to an example Kafka topic that we have configured within the Qwak enviornment. The topic produces Transaction events that have the following JSON schema:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"timestamp\": \"2024-01-01T00:00:00\",\n",
    "    \"user_id\": \"eab027fc-3a65-4d02-95af-d8754e27b7d0\",\n",
    "    \"transaction_amount\": 151.76\n",
    "}\n",
    "```\n",
    "In addition to ingesting the transaction events, we will also define transformations on the ingestion pipeline that will generate aggregated calculations on the `transaction_amount` column to produce additional data points for our Machine Learning model. \n",
    "\n",
    "\n",
    "## To create the ingestion pipeline, we will walk through the following steps:\n",
    "1. Configure the deserialization function to handle the incoming events with proper schema\n",
    "2. Define the Kafka Streaming Data Source\n",
    "3. Define Kakfa Streaming Feature Set Pipeline including metadata and streaming aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb8abb-baa9-4e06-b859-714c72a3fd3a",
   "metadata": {},
   "source": [
    "## Defining the Deserializtion Function\n",
    "\n",
    "As mentioned above, Qwak provides two methods for handling the schema of incoming events:\n",
    "- Generic Deserializer\n",
    "- Custom Deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793973c-dc62-4a67-a9a4-2d00320c744e",
   "metadata": {},
   "source": [
    "# Generic Deserializer\n",
    "\n",
    "The Generic Deserializer will automatically infer and parse the incoming Kafka message based on event schema. The Generic Deserializer is simple to use, but is best used for cases of non-complex Kafka messages, like the schema we have mentioned above. The Generic Deserializer makes a few assumptions:\n",
    "- The schema of the message/event is either AVRO or JSON\n",
    "- The message data to be parsed out will be stored under the `value` field\n",
    "- Compatible data types are in accordance to Apache Spark data types - https://spark.apache.org/docs/3.1.1/sql-ref-datatypes.html\n",
    "\n",
    "Here is a sample configuration using the message schema above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23cdbdce-4e9d-41ac-91e5-e6e39dcceca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.data_sources import GenericDeserializer, MessageFormat\n",
    "\n",
    "\n",
    "event_schema = {\n",
    "  \"type\": \"struct\", \n",
    "  \"fields\": [ \n",
    "    { \n",
    "      \"metadata\": {}, \n",
    "      \"name\": \"timestamp\",\n",
    "      \"nullable\": True,\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "      \"metadata\": {},\n",
    "      \"name\": \"key\",\n",
    "      \"nullable\": True,\n",
    "      \"type\": \"integer\"\n",
    "    },\n",
    "    {\n",
    "      \"metadata\": {},\n",
    "      \"name\": \"value\",\n",
    "      \"nullable\": True,\n",
    "      \"type\": \"integer\",\n",
    "      \"fields\": [\n",
    "          {\n",
    "              \"metadata\": {},\n",
    "              \"name\": \"timestamp\",\n",
    "              \"nullable\": True,\n",
    "              \"type\": \"string\"\n",
    "          },\n",
    "          {\n",
    "              \"metadata\": {},\n",
    "              \"name\": \"user_id\",\n",
    "              \"nullable\": True,\n",
    "              \"type\": \"string\"\n",
    "          },\n",
    "          {\n",
    "              \"metadata\": {},\n",
    "              \"name\": \"transaction_amount\",\n",
    "              \"nullable\": True,\n",
    "              \"type\": \"decimal\"\n",
    "          }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "deserializer = GenericDeserializer(message_format=MessageFormat.JSON, schema=str(event_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ef07a-bee1-4c75-ae25-c01f0780bca1",
   "metadata": {},
   "source": [
    "# Custom Deserializer\n",
    "\n",
    "The custom deserializer allows you to specify a python function that can be used to interpret, parse and format the incoming event. The custom deserializer function should accept a Pyspark Dataframe as an input return a Pyspark Dataframe as an output. The custom deserializer allows you to handle complex schemas in an easier way, allowing you to manipulate the keys before passing into the ingestion pipeline. \n",
    "\n",
    "\n",
    "In the function you will define an output schema using the Spark StructType and StructField objects. Then using the from_json Spark function, you will parse the input json into the schema you defined.\n",
    "\n",
    "Here is a sample deserialization function and CustomDeserializer implementation using the same transaction event schema above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfafd890-f13a-4c6c-a636-a8ae9bfd744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.data_sources import KafkaSource, CustomDeserializer\n",
    "from pyspark.sql.functions import col, from_json\n",
    "def deser_function(df):\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"timestamp\", TimestampType()),\n",
    "            StructField(\"user_id\", StringType()),\n",
    "            StructField(\"transaction_amount\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "    deserialized = df.select(col(\"partition\"), col(\"topic\"), col(\"offset\"),\n",
    "                             from_json(col(\"value\").cast(StringType()), schema).alias(\"data\")\n",
    "                             ).select(col(\"data.*\"), col(\"partition\"), col(\"topic\"), col(\"offset\")) \\\n",
    "        .select(col(\"partition\"), col(\"topic\"), col(\"offset\"), col(\"timestamp\"), col(\"user_id\"),\n",
    "                col(\"transaction_amount\"))\n",
    "\n",
    "    return deserialized\n",
    "deserializer = CustomDeserializer(function=deser_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0eb25-952f-4fb6-a6f4-42a115f7a2a0",
   "metadata": {},
   "source": [
    "# Define the DataSource\n",
    "\n",
    "Now that we have a function to handle the incoming event schemas, we can define our data source. We'll use the KafkaSource from the Qwak SDK. When defining the Data source using the Qwak SDK, we'll include the deserialization function in the same file as the Data Source definition. In this example, we'll use the custom deserializer defined above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba86cee5-349c-47a4-9bb4-aab52b3690e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_source.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_source.py\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from qwak.feature_store.data_sources import KafkaSource, CustomDeserializer\n",
    "from pyspark.sql.functions import col, from_json\n",
    "def deser_function(df):\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"timestamp\", TimestampType()),\n",
    "            StructField(\"user_id\", StringType()),\n",
    "            StructField(\"transaction_amount\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "    deserialized = df.select(col(\"partition\"), col(\"topic\"), col(\"offset\"),\n",
    "                             from_json(col(\"value\").cast(StringType()), schema).alias(\"data\")\n",
    "                             ).select(col(\"data.*\"), col(\"partition\"), col(\"topic\"), col(\"offset\")) \\\n",
    "        .select(col(\"partition\"), col(\"topic\"), col(\"offset\"), col(\"timestamp\"), col(\"user_id\"),\n",
    "                col(\"transaction_amount\"))\n",
    "\n",
    "    return deserialized\n",
    "deserializer = CustomDeserializer(function=deser_function)\n",
    "\n",
    "# replace with your own Kafka brokers\n",
    "bootstrap_servers = 'b-2.qwak-cluster.za36zh.c6.kafka.us-east-1.amazonaws.com:9094,b-1.qwak-cluster.za36zh.c6.kafka.us-east-1.amazonaws.com:9094,b-3.qwak-cluster.za36zh.c6.kafka.us-east-1.amazonaws.com:9094'\n",
    "\n",
    "# replace with your own Kafka passthrough configurations\n",
    "passthrough_configs = {\n",
    "    \"qwak.online.maxOffsetsPerTrigger\": \"200000\",\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "}\n",
    "\n",
    "\n",
    "kafka_source = KafkaSource(\n",
    "    # metadata name of the DataSource within Qwak\n",
    "    name=\"transactions\",\n",
    "    # metadata description of the DataSource within Qwak\n",
    "    description=\"Transaction Test Source\",\n",
    "    # Bootstrap servers of the Kafka client to be connected to\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    # Kafka topic to be ingested\n",
    "    subscribe=\"transactions\",\n",
    "    # Deserialization function described above\n",
    "    deserialization=deserializer,\n",
    "    # Passthrough Kafka configs for offset settings, defined above\n",
    "    passthrough_configs=passthrough_configs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf607af-77d4-4600-aaf0-cd9100c77632",
   "metadata": {},
   "source": [
    "### Additional Considerations for Registering Data Sources\n",
    "\n",
    "When registering Data Sources in Qwak, it's essential to ensure that the underlying data store is accessible by the platform. Depending on your deployment model (Hybrid or SaaS), there are different ways to grant Qwak access to your data.\n",
    "\n",
    "#### Accessing AWS Resources:\n",
    "\n",
    "If your data is stored in AWS services, you can grant access to Qwak using an IAM role ARN. For detailed instructions, refer to our documentation on [Accessing AWS Resources with IAM Role](https://docs-saas.qwak.com/docs/accessing-aws-resources-with-iam-role).\n",
    "\n",
    "#### Using Qwak Secrets:\n",
    "\n",
    "Alternatively, you can pass the credentials as Qwak Secrets. This approach provides a secure way to manage and authenticate access to your data. For more information, see [Qwak Secret Management](https://docs-saas.qwak.com/docs/secret-management).\n",
    "\n",
    "For more information about the types of Data Sources supported by Qwak, refer to our documentation:\n",
    "- [Batch Data Sources](https://docs-saas.qwak.com/docs/batch-data-sources)\n",
    "- [Streaming Data Sources](https://docs-saas.qwak.com/docs/streaming-data-sources)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sampling Data from the Data Source\n",
    "\n",
    "It's important to note that the data source cannot be used as a query engine independently (for now). Instead, it serves as a sampling mechanism to verify that the data is being queried properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a36ed0-3c76-414e-8d62-455e61302902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source Data Types:\n",
      "\n",
      "partition                      int64\n",
      "topic                         object\n",
      "offset                         int64\n",
      "timestamp             datetime64[ns]\n",
      "user_id                       object\n",
      "transaction_amount             int64\n",
      "dtype: object\n",
      "\n",
      "Data Source Sample :\n",
      "\n",
      "   partition         topic     offset               timestamp                               user_id  transaction_amount\n",
      "0          0  transactions  163191965 2024-04-16 19:39:49.972  02d7f29f-90b3-47f2-9952-126bdd18c378             1788481\n",
      "1          0  transactions  163191966 2024-04-16 19:39:50.012  5abb1d49-a1aa-4f2a-92a2-6f537ec7421d             1788483\n",
      "2          0  transactions  163191967 2024-04-16 19:39:50.052  72878824-25ec-4a5b-a4db-6ed18009691d             1788485\n",
      "3          0  transactions  163191968 2024-04-16 19:39:50.092  455d686b-d8f2-4e2c-aa6b-851e9b3de446             1788487\n",
      "4          0  transactions  163191969 2024-04-16 19:39:50.132  15b6edce-9b91-4c1f-aae9-2247d0d1a427             1788489\n",
      "5          0  transactions  163191970 2024-04-16 19:39:50.172  73142300-493b-45f5-b355-3dadd54b0c13             1788491\n",
      "6          0  transactions  163191971 2024-04-16 19:39:50.212  8410fd1f-4f76-4375-8125-df3fe383cc60             1788493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run data_source.py\n",
    "\n",
    "df_sample = kafka_source.get_sample()\n",
    "print (f\"Data Source Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"Data Source Sample :\\n\\n{df_sample.head(7).to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863fa8c-bbee-47e7-a6ed-6e4107aabfc1",
   "metadata": {},
   "source": [
    "## Registering the Data Source with the Qwak Platform\n",
    "\n",
    "After verifying that the Data Source returns the desired results, the next step is to register it with the Qwak Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13698b4-7614-4fcb-a8d6-19dce66f319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m Finding Entities to register (0:00:00.04)\n",
      "üëÄ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "üëÄ Found 1 Data Sources\n",
      "Validating 'transactions' data source\n",
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m  (0:00:03.83)\n",
      "‚úÖ Validation completed successfully, got data source columns:\n",
      "column name         type\n",
      "------------------  ---------\n",
      "partition           int\n",
      "topic               string\n",
      "offset              bigint\n",
      "timestamp           timestamp\n",
      "user_id             string\n",
      "transaction_amount  int\n",
      "Update existing Data Source 'transactions' from source file '/Users/hudsonbuzby/dev/qwak-examples/credit_risk_streaming/data_source.py'?\n",
      "continue? [y/N]: ----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "üëÄ Found 0 Feature Set(s)\n"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | qwak features register -p data_source.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6685abf-0d6a-407a-8729-6e7ab94a7e96",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Creating the Streaming Feature Set from the Kafka Data Source\n",
    "\n",
    "The Streaming Feature Set is creating using the @streaming.feature_set decorator from the Qwak SDK. To create a streaming feature set, you'll need to define the following properties :\n",
    "\n",
    "- **@Metadata:** Includes feature set name, key, data sources, and the timestamp column used for indexing.\n",
    "- **@streaming.Metadata:** Additional metadata including display name, feature set description, and feature set owner.\n",
    "- **Scheduling Expression:** The rate at which the offline and online store are refreshed\n",
    "- **Cluster Type:** Specifies the resources to use for running both the online and offline ingestion job.\n",
    "- **Backfill:** Determines how far back in time the Feature Set should ingest data.\n",
    "- **Transformation:** Can be SQL-based, UDF-based (currently Koalas), or Qwak Aggregation logic for data transformation\n",
    "\n",
    "For more information related to the resources used in the cluster templates, check out [Qwak docs](https://docs-saas.qwak.com/docs/instance-sizes#feature-store).\n",
    "\n",
    "\n",
    "Let's take a look at a sample streaming feature set pipeline using the Aggregation capabilities of the Qwak Streaming Feature Store                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac76d7a-d600-4950-9c75-04125992b547",
   "metadata": {},
   "source": [
    "## Defining the Base Feature Set Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fadd1-d3a1-4ab0-89bf-a08563ff1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.feature_sets import streaming\n",
    "\n",
    "# Streaming Feature Set decorator from Qwak\n",
    "@streaming.feature_set(\n",
    "    # Name of the feature set\n",
    "    name=\"credit-risk-streaming\",\n",
    "    # Kafka Data Source created in previous step\n",
    "    data_sources=[\"transactions\"],\n",
    "    # Entity or Key Column, Qwak will use this to distinguish uniqueness in the feature set\n",
    "    key=\"user_id\",\n",
    "    # Timestamp column, Qwak will use this to deduplicate records and schedule trigger windows\n",
    "    timestamp_column_name=\"timestamp\"),\n",
    "    # A crontab definition of the the offline ingestion policy - which affects the data freshness the offline store. defaults to */30 * * * * (every 30 minutes)\n",
    "    offline_scheduling_policy=\"0 * * * *\",\n",
    "    # Defines the online ingestion policy - which affects the data freshness of the online store. Defaults to 5 seconds.\n",
    "    online_trigger_interval=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ed037-9592-46ae-9c1f-639aee812bd6",
   "metadata": {},
   "source": [
    "## Defining the Feature Set MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d9e3b-f6c5-41a9-b0c9-a0da332f3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.feature_sets import streaming\n",
    "\n",
    "# Streaming Feature Set decorator from Qwak\n",
    "@streaming.metadata(\n",
    "        # Display name to be used by the UI\n",
    "        display_name=\"Credit Risk Streaming\",\n",
    "        # Description to be displayed in the UI\n",
    "        description=\"streaming transaction aggregations over 1,15,30,60 minutes\",\n",
    "        # User owner of the feature set\n",
    "        owner=\"hudson@qwak.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4f12a-5712-465e-8fa2-c406b77bd61d",
   "metadata": {},
   "source": [
    "## Defining the Feature Set Cluster Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1facb1-d381-42fa-858e-8142670f6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.feature_sets.execution_spec import ClusterTemplate\n",
    "\n",
    "# Streaming Feature Set decorator from Qwak\n",
    "@streaming.execution_specification(\n",
    "    # Cluster resources to be used for Online Ingestion\n",
    "    online_cluster_template=ClusterTemplate.SMALL,\n",
    "    # Cluster resources to be used for Offline Ingestion\n",
    "    offline_cluster_template=ClusterTemplate.MEDIUM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3aa2d4-d304-4332-8aba-6f89733be082",
   "metadata": {},
   "source": [
    "## Defining the Feature Set Transformation\n",
    "\n",
    "You can define the Streaming Feature set transformation logic as a SparkSQL query, a series of UDF transformations, or Aggregations using the Qwak SDK. At the end of your Streaming Feature Set definition, you will define a function with no arguments that returns a SparkSqlTransformation. For a simple SQL transformation, you can do something like the following command. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda54048-a222-4182-8cae-b6c87f80e713",
   "metadata": {},
   "source": [
    "### SQL Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa6d57-8989-453e-ae21-0d1517e6ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.feature_sets.transformations import SparkSqlTransformation\n",
    "\n",
    "def transform():\n",
    "    return SparkSqlTransformation(sql=\"\"\"\n",
    "        SELECT timestamp,\n",
    "        user_id,\n",
    "        transaction_amount\n",
    "        FROM transactions\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c4f29-ac19-4461-98e8-8a4296e319a9",
   "metadata": {},
   "source": [
    "### Defining the Feature Set Transformation Logic with Aggregation\n",
    "\n",
    "Qwak provides advanced support for Streaming Aggregations using the QwakAggregation functionality of the Qwak SDK. An aggregation takes a Data Source column as an argument. The column must be defined in the schema of the Data Source. For aggregations that are `last_n` or percentile based, you can pass in an additional argument that specifies the quantity of last_n or the percentile to be aggregated. \n",
    "\n",
    "The following list is the aggregations currently supported by Qwak\n",
    "\n",
    "\n",
    "- **SUM** - a sum of column, for example, QwakAggregation.sum(\"transaction_amount\")\n",
    "- **COUNT** - count (not distinct), a column is specified for API uniformity. for example, QwakAggregation.count(\"transaction_amount\")\n",
    "- **AVERAGE** - mean value, for example QwakAggregation.avg(\"transaction_amount\")\n",
    "- **MIN** - minimum value, for example QwakAggregation.min(\"transaction_amount\")\n",
    "- **MAX** - maximum value, for example QwakAggregation.max(\"transaction_amount\")\n",
    "- **BOOLEAN OR** - boolean or, defined over a boolean column, for example QwakAggregation.boolean_or(\"is_remote\")\n",
    "- **BOOLEAN AND** - boolean and, defined over a boolean column, for example QwakAggregation.boolean_and(\"is_remote\")\n",
    "- **Sample Variance** - QwakAggregation.sample_variance(\"transaction_amount\")\n",
    "- **Sample STDEV** - QwakAggregation.sample_stdev(\"transaction_amount\")\n",
    "- **Population Variance** - QwakAggregation.population_variance(\"transaction_amount\")\n",
    "- **Population STDEV** - QwakAggregation.population_stdev(\"transaction_amount\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec7f28-4c05-4bb3-ac32-bfbd2d525375",
   "metadata": {},
   "source": [
    "Take a look at an example aggregation transformation defined below. This aggregation takes our transaction data source, and takes the sum, count, max, standard deviation, last 5 records, last 5 disctinct records, and the 50th percentile aggregation amounts. We also specify the window amount after the aggregation, which determines how many windows of time we will aggregate across. So for the 7 aggregations defined below, we are able to create 28 aggregation features all in the same application! You can also add alias notation to rename some of the aggregations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef3d58-6f98-4dba-b0bd-a4ca9e3cba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.feature_store.feature_sets.transformations import SparkSqlTransformation, QwakAggregation\n",
    "\n",
    "\n",
    "def transform():\n",
    "    return SparkSqlTransformation(sql=\"\"\"SELECT * FROM transactions\"\"\")\\\n",
    "            .aggregate(QwakAggregation.sum(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.count(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.max(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.sample_stdev(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.last_n(\"transaction_amount\", 5))\\\n",
    "            .aggregate(QwakAggregation.last_distinct_n(\"transaction_amount\", 5))\\\n",
    "            .aggregate(QwakAggregation.percentile(\"transaction_amount\", 50)\\\n",
    "                    .alias(\"median_transaction_amount\"))\\\n",
    "            .by_windows(\"1 minute\", \"15 minutes\", \"30 minutes\", \"1 hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09592c8b-4694-4100-96b3-97ef38a380b1",
   "metadata": {},
   "source": [
    "# Build the Streaming Feature Set\n",
    "\n",
    "Now, lets put the streaming feature set definition, metadata, cluster spec, and transformation logic all together so we can define the feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115c8568-e40e-48f4-99fe-97c9d8f90f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streaming_feature_set.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streaming_feature_set.py\n",
    "\n",
    "from qwak.feature_store.feature_sets import streaming\n",
    "from qwak.feature_store.feature_sets.execution_spec import ClusterTemplate\n",
    "from qwak.feature_store.feature_sets.transformations import SparkSqlTransformation, QwakAggregation\n",
    "\n",
    "\n",
    "@streaming.feature_set(\n",
    "    name=\"credit-risk-streaming\",\n",
    "    data_sources=[\"transactions\"],\n",
    "    key=\"user_id\",\n",
    "    timestamp_column_name=\"timestamp\",\n",
    "    offline_scheduling_policy=\"0 * * * *\",\n",
    "    online_trigger_interval=30\n",
    ")\n",
    "@streaming.metadata(\n",
    "        display_name=\"Credit Risk Streaming\",\n",
    "        description=\"streaming transaction aggregations over 1,15,30,60 minutes\",\n",
    "        owner=\"hudson@qwak.com\"\n",
    ")\n",
    "\n",
    "@streaming.execution_specification(\n",
    "    online_cluster_template=ClusterTemplate.SMALL,\n",
    "    offline_cluster_template=ClusterTemplate.MEDIUM,\n",
    ")\n",
    "def transform():\n",
    "    return SparkSqlTransformation(sql=\"\"\"SELECT * FROM transactions\"\"\")\\\n",
    "            .aggregate(QwakAggregation.sum(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.count(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.max(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.sample_stdev(\"transaction_amount\"))\\\n",
    "            .aggregate(QwakAggregation.last_n(\"transaction_amount\", 5))\\\n",
    "            .aggregate(QwakAggregation.last_distinct_n(\"transaction_amount\", 5))\\\n",
    "            .aggregate(QwakAggregation.percentile(\"transaction_amount\", 50)\\\n",
    "                    .alias(\"median_transaction_amount\"))\\\n",
    "            .by_windows(\"1 minute\", \"15 minutes\", \"30 minutes\", \"1 hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc91602-d87f-4886-9ae0-ad9046f875f7",
   "metadata": {},
   "source": [
    "## Sampling the Data Source and Printing Data and Data Types\n",
    "\n",
    "If your data source takes more than 5 minutes to query or fetch a sample of the data (for example, due to long-running queries), your sampling process may fail with a timeout error. In such cases, you can skip validation during registration with Qwak and proceed to register your feature set, allowing it to run an ingestion job.\n",
    "\n",
    "### Note:\n",
    "The sampling process is essential for verifying that the data is queried properly. However, if it takes too long, you can proceed with the registration without validation and rely on the ingestion job to ensure data correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6d9c2f-fae4-4de7-94f8-c79de244489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source Data Types:\n",
      "\n",
      "user_id                                    object\n",
      "count_transaction_amount_1h                 int64\n",
      "last_distinct_5_transaction_amount_1h      object\n",
      "sum_transaction_amount_1h                   int64\n",
      "sample_stdev_transaction_amount_1h        float64\n",
      "median_transaction_amount_1h                int64\n",
      "last_5_transaction_amount_1h               object\n",
      "max_transaction_amount_1h                   int64\n",
      "count_transaction_amount_30m                int64\n",
      "last_distinct_5_transaction_amount_30m     object\n",
      "sum_transaction_amount_30m                  int64\n",
      "sample_stdev_transaction_amount_30m       float64\n",
      "median_transaction_amount_30m               int64\n",
      "last_5_transaction_amount_30m              object\n",
      "max_transaction_amount_30m                  int64\n",
      "count_transaction_amount_1m                 int64\n",
      "last_distinct_5_transaction_amount_1m      object\n",
      "sum_transaction_amount_1m                   int64\n",
      "sample_stdev_transaction_amount_1m        float64\n",
      "median_transaction_amount_1m                int64\n",
      "last_5_transaction_amount_1m               object\n",
      "max_transaction_amount_1m                   int64\n",
      "count_transaction_amount_15m                int64\n",
      "last_distinct_5_transaction_amount_15m     object\n",
      "sum_transaction_amount_15m                  int64\n",
      "sample_stdev_transaction_amount_15m       float64\n",
      "median_transaction_amount_15m               int64\n",
      "last_5_transaction_amount_15m              object\n",
      "max_transaction_amount_15m                  int64\n",
      "dtype: object\n",
      "\n",
      "Data Source Sample :\n",
      "\n",
      "                                user_id  count_transaction_amount_1h  \\\n",
      "0  a591a687-65f6-4c36-841a-2d965816f880                            5   \n",
      "1  5abb1d49-a1aa-4f2a-92a2-6f537ec7421d                            6   \n",
      "2  5f97899e-de8a-4527-b48b-633ec77dc722                            5   \n",
      "3  a8af33ad-23b9-48f9-8fe2-8b3d1a0a6829                            5   \n",
      "4  5d1d338c-4ac9-447b-b388-823951ede8f6                            5   \n",
      "5  162d2517-ce7a-46aa-8e51-3e38a1de5d22                            5   \n",
      "6  72878824-25ec-4a5b-a4db-6ed18009691d                            6   \n",
      "7  181153df-581b-4cf9-9649-aee2eedb25d5                            5   \n",
      "8  a8cd57ca-0199-46e3-a44e-a9d5927edd7a                            5   \n",
      "9  46e361c2-9c09-4bea-bfa6-178b80bc5aec                            5   \n",
      "\n",
      "           last_distinct_5_transaction_amount_1h  sum_transaction_amount_1h  \\\n",
      "0  [1789510, 1789460, 1789410, 1789360, 1789310]                    8947050   \n",
      "1  [1789533, 1789483, 1789433, 1789383, 1789333]                   10736448   \n",
      "2  [1789514, 1789464, 1789414, 1789364, 1789314]                    8947070   \n",
      "3  [1789518, 1789468, 1789418, 1789368, 1789318]                    8947090   \n",
      "4  [1789534, 1789484, 1789434, 1789384, 1789334]                    8947170   \n",
      "5  [1789511, 1789461, 1789411, 1789361, 1789311]                    8947055   \n",
      "6  [1789535, 1789485, 1789435, 1789385, 1789335]                   10736460   \n",
      "7  [1789540, 1789490, 1789440, 1789390, 1789340]                    8947200   \n",
      "8  [1789507, 1789457, 1789407, 1789357, 1789307]                    8947035   \n",
      "9  [1789523, 1789473, 1789423, 1789373, 1789323]                    8947115   \n",
      "\n",
      "   sample_stdev_transaction_amount_1h  median_transaction_amount_1h  \\\n",
      "0                           79.056942                       1789410   \n",
      "1                           93.541435                       1789383   \n",
      "2                           79.056942                       1789414   \n",
      "3                           79.056942                       1789418   \n",
      "4                           79.056942                       1789434   \n",
      "5                           79.056942                       1789411   \n",
      "6                           93.541435                       1789385   \n",
      "7                           79.056942                       1789440   \n",
      "8                           79.056942                       1789407   \n",
      "9                           79.056942                       1789423   \n",
      "\n",
      "                    last_5_transaction_amount_1h  max_transaction_amount_1h  \\\n",
      "0  [1789510, 1789460, 1789410, 1789360, 1789310]                    1789510   \n",
      "1  [1789533, 1789483, 1789433, 1789383, 1789333]                    1789533   \n",
      "2  [1789514, 1789464, 1789414, 1789364, 1789314]                    1789514   \n",
      "3  [1789518, 1789468, 1789418, 1789368, 1789318]                    1789518   \n",
      "4  [1789534, 1789484, 1789434, 1789384, 1789334]                    1789534   \n",
      "5  [1789511, 1789461, 1789411, 1789361, 1789311]                    1789511   \n",
      "6  [1789535, 1789485, 1789435, 1789385, 1789335]                    1789535   \n",
      "7  [1789540, 1789490, 1789440, 1789390, 1789340]                    1789540   \n",
      "8  [1789507, 1789457, 1789407, 1789357, 1789307]                    1789507   \n",
      "9  [1789523, 1789473, 1789423, 1789373, 1789323]                    1789523   \n",
      "\n",
      "   count_transaction_amount_30m  \\\n",
      "0                             5   \n",
      "1                             6   \n",
      "2                             5   \n",
      "3                             5   \n",
      "4                             5   \n",
      "5                             5   \n",
      "6                             6   \n",
      "7                             5   \n",
      "8                             5   \n",
      "9                             5   \n",
      "\n",
      "          last_distinct_5_transaction_amount_30m  ...  \\\n",
      "0  [1789510, 1789460, 1789410, 1789360, 1789310]  ...   \n",
      "1  [1789533, 1789483, 1789433, 1789383, 1789333]  ...   \n",
      "2  [1789514, 1789464, 1789414, 1789364, 1789314]  ...   \n",
      "3  [1789518, 1789468, 1789418, 1789368, 1789318]  ...   \n",
      "4  [1789534, 1789484, 1789434, 1789384, 1789334]  ...   \n",
      "5  [1789511, 1789461, 1789411, 1789361, 1789311]  ...   \n",
      "6  [1789535, 1789485, 1789435, 1789385, 1789335]  ...   \n",
      "7  [1789540, 1789490, 1789440, 1789390, 1789340]  ...   \n",
      "8  [1789507, 1789457, 1789407, 1789357, 1789307]  ...   \n",
      "9  [1789523, 1789473, 1789423, 1789373, 1789323]  ...   \n",
      "\n",
      "   median_transaction_amount_1m  \\\n",
      "0                       1789410   \n",
      "1                       1789383   \n",
      "2                       1789414   \n",
      "3                       1789418   \n",
      "4                       1789434   \n",
      "5                       1789411   \n",
      "6                       1789385   \n",
      "7                       1789440   \n",
      "8                       1789407   \n",
      "9                       1789423   \n",
      "\n",
      "                    last_5_transaction_amount_1m  max_transaction_amount_1m  \\\n",
      "0  [1789510, 1789460, 1789410, 1789360, 1789310]                    1789510   \n",
      "1  [1789533, 1789483, 1789433, 1789383, 1789333]                    1789533   \n",
      "2  [1789514, 1789464, 1789414, 1789364, 1789314]                    1789514   \n",
      "3  [1789518, 1789468, 1789418, 1789368, 1789318]                    1789518   \n",
      "4  [1789534, 1789484, 1789434, 1789384, 1789334]                    1789534   \n",
      "5  [1789511, 1789461, 1789411, 1789361, 1789311]                    1789511   \n",
      "6  [1789535, 1789485, 1789435, 1789385, 1789335]                    1789535   \n",
      "7  [1789540, 1789490, 1789440, 1789390, 1789340]                    1789540   \n",
      "8  [1789507, 1789457, 1789407, 1789357, 1789307]                    1789507   \n",
      "9  [1789523, 1789473, 1789423, 1789373, 1789323]                    1789523   \n",
      "\n",
      "  count_transaction_amount_15m         last_distinct_5_transaction_amount_15m  \\\n",
      "0                            5  [1789510, 1789460, 1789410, 1789360, 1789310]   \n",
      "1                            6  [1789533, 1789483, 1789433, 1789383, 1789333]   \n",
      "2                            5  [1789514, 1789464, 1789414, 1789364, 1789314]   \n",
      "3                            5  [1789518, 1789468, 1789418, 1789368, 1789318]   \n",
      "4                            5  [1789534, 1789484, 1789434, 1789384, 1789334]   \n",
      "5                            5  [1789511, 1789461, 1789411, 1789361, 1789311]   \n",
      "6                            6  [1789535, 1789485, 1789435, 1789385, 1789335]   \n",
      "7                            5  [1789540, 1789490, 1789440, 1789390, 1789340]   \n",
      "8                            5  [1789507, 1789457, 1789407, 1789357, 1789307]   \n",
      "9                            5  [1789523, 1789473, 1789423, 1789373, 1789323]   \n",
      "\n",
      "   sum_transaction_amount_15m sample_stdev_transaction_amount_15m  \\\n",
      "0                     8947050                           79.056942   \n",
      "1                    10736448                           93.541435   \n",
      "2                     8947070                           79.056942   \n",
      "3                     8947090                           79.056942   \n",
      "4                     8947170                           79.056942   \n",
      "5                     8947055                           79.056942   \n",
      "6                    10736460                           93.541435   \n",
      "7                     8947200                           79.056942   \n",
      "8                     8947035                           79.056942   \n",
      "9                     8947115                           79.056942   \n",
      "\n",
      "   median_transaction_amount_15m  \\\n",
      "0                        1789410   \n",
      "1                        1789383   \n",
      "2                        1789414   \n",
      "3                        1789418   \n",
      "4                        1789434   \n",
      "5                        1789411   \n",
      "6                        1789385   \n",
      "7                        1789440   \n",
      "8                        1789407   \n",
      "9                        1789423   \n",
      "\n",
      "                   last_5_transaction_amount_15m  max_transaction_amount_15m  \n",
      "0  [1789510, 1789460, 1789410, 1789360, 1789310]                     1789510  \n",
      "1  [1789533, 1789483, 1789433, 1789383, 1789333]                     1789533  \n",
      "2  [1789514, 1789464, 1789414, 1789364, 1789314]                     1789514  \n",
      "3  [1789518, 1789468, 1789418, 1789368, 1789318]                     1789518  \n",
      "4  [1789534, 1789484, 1789434, 1789384, 1789334]                     1789534  \n",
      "5  [1789511, 1789461, 1789411, 1789361, 1789311]                     1789511  \n",
      "6  [1789535, 1789485, 1789435, 1789385, 1789335]                     1789535  \n",
      "7  [1789540, 1789490, 1789440, 1789390, 1789340]                     1789540  \n",
      "8  [1789507, 1789457, 1789407, 1789357, 1789307]                     1789507  \n",
      "9  [1789523, 1789473, 1789423, 1789373, 1789323]                     1789523  \n",
      "\n",
      "[10 rows x 29 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run streaming_feature_set.py\n",
    "\n",
    "df_sample = transform.get_sample()\n",
    "print (f\"Data Source Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"Data Source Sample :\\n\\n{df_sample}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fce29e-81ae-46d4-a5fa-4c1865a62345",
   "metadata": {},
   "source": [
    "## Registering the Streaming Feature Set\n",
    "\n",
    "Now that your Streaming Aggregation Feature Set is defined, you can register it with Qwak so that it continuously runs and ingests data. Run the following command to complete the registration process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d44cdb9-819a-4953-b4bd-dd2638027cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m Finding Entities to register (0:00:00.14)\n",
      "üëÄ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "üëÄ Found 0 Data Sources\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34m‚úÖ\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "üëÄ Found 1 Feature Set(s)\n",
      "Update existing feature set 'credit-risk-streaming' from source file '/Users/hudsonbuzby/dev/qwak-examples/credit_risk_streaming/streaming_feature_set.py'?\n",
      "continue? [y/N]: Validating 'credit-risk-streaming' feature set\n",
      "\u001b[34m‚†è\u001b[0m  (0:00:17.52)\u001b[?25h^C\n",
      "\u001b[K\u001b[?25hm  (0:00:17.77)"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | qwak features register -p streaming_feature_set.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f97a27-aaa0-4480-b2d3-5b2919c13b04",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Consuming Features from the Offline Feature Store (Training/Batch Inference)\n",
    "\n",
    "To retrieve features from the Offline Feature Store for training or batch inference, you can use the `get_feature_values` method:\n",
    "\n",
    "1. **get_feature_values**:\n",
    "   - Fetches records associated with the provided set of keys, inserted at a specific timestamp.\n",
    "   - Query date must fall between the start and end timestamp.\n",
    "\n",
    "\n",
    "You can read more about the feature store retrieval methods here [our docs](https://docs-saas.qwak.com/docs/getting-features-for-training#get-feature-values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "434cf2f1-39b5-4ba2-a0fd-ae03ea500826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label                timestamp                               user_id  \\\n",
      "0       0  2024-03-27 15:21:32.000  149ca9e4-0e4a-48e8-b9e2-263b23d371c0   \n",
      "1       0  2024-03-20 15:21:32.000  149ca9e4-0e4a-48e8-b9e2-263b23d371c0   \n",
      "2       0  2024-03-27 15:21:32.000  742f09a6-b88b-4518-aa7b-431aa8ae8a16   \n",
      "3       0  2024-03-27 15:21:32.000  72878824-25ec-4a5b-a4db-6ed18009691d   \n",
      "4       0  2024-03-27 15:21:32.000  8410fd1f-4f76-4375-8125-df3fe383cc60   \n",
      "..    ...                      ...                                   ...   \n",
      "95      0  2024-03-20 15:21:32.000  95ec0c53-4e27-4490-b85f-1448de70fc26   \n",
      "96      0  2024-03-20 15:21:32.000  ca2da30a-ffde-4334-87e5-7dae3bebb5db   \n",
      "97      0  2024-03-20 15:21:32.000  89161639-e300-4789-afee-d675cfa383e1   \n",
      "98      0  2024-03-27 15:21:32.000  5a114060-1038-4dc6-a038-3b7423cf2c16   \n",
      "99      0  2024-03-27 15:21:32.000  2c39a950-04e0-43d5-bbba-bea628309b0c   \n",
      "\n",
      "    transaction-aggregates-demo.count_transaction_amount_1m  \\\n",
      "0                                                  60         \n",
      "1                                                  60         \n",
      "2                                                  60         \n",
      "3                                                  60         \n",
      "4                                                  60         \n",
      "..                                                ...         \n",
      "95                                                 60         \n",
      "96                                                 60         \n",
      "97                                                 60         \n",
      "98                                                 60         \n",
      "99                                                 60         \n",
      "\n",
      "   transaction-aggregates-demo.last_distinct_5_transaction_amount_1m  \\\n",
      "0       [1500172, 1500122, 1500072, 1500022, 1499972]                  \n",
      "1       [3239672, 3239622, 3239572, 3239522, 3239472]                  \n",
      "2       [1500166, 1500116, 1500066, 1500016, 1499966]                  \n",
      "3       [1500185, 1500135, 1500085, 1500035, 1499985]                  \n",
      "4       [1500193, 1500143, 1500093, 1500043, 1499993]                  \n",
      "..                                                ...                  \n",
      "95      [3239659, 3239609, 3239559, 3239509, 3239459]                  \n",
      "96      [3239680, 3239630, 3239580, 3239530, 3239480]                  \n",
      "97      [3239636, 3239586, 3239536, 3239486, 3239436]                  \n",
      "98      [1500196, 1500146, 1500096, 1500046, 1499996]                  \n",
      "99      [1500197, 1500147, 1500097, 1500047, 1499997]                  \n",
      "\n",
      "    transaction-aggregates-demo.sum_transaction_amount_1m  \\\n",
      "0                                            89921820       \n",
      "1                                           194291820       \n",
      "2                                            89921460       \n",
      "3                                            89922600       \n",
      "4                                            89923080       \n",
      "..                                                ...       \n",
      "95                                          194291040       \n",
      "96                                          194292300       \n",
      "97                                          194289660       \n",
      "98                                           89923260       \n",
      "99                                           89923320       \n",
      "\n",
      "    transaction-aggregates-demo.sample_stdev_transaction_amount_1m  \\\n",
      "0                                          873.212460                \n",
      "1                                          873.212460                \n",
      "2                                          873.212460                \n",
      "3                                          873.212460                \n",
      "4                                          873.212460                \n",
      "..                                                ...                \n",
      "95                                         873.212462                \n",
      "96                                         873.212460                \n",
      "97                                         873.212460                \n",
      "98                                         873.212460                \n",
      "99                                         873.212460                \n",
      "\n",
      "    transaction-aggregates-demo.median_transaction_amount_1m  \\\n",
      "0                                             1498672          \n",
      "1                                             3238172          \n",
      "2                                             1498666          \n",
      "3                                             1498685          \n",
      "4                                             1498693          \n",
      "..                                                ...          \n",
      "95                                            3238159          \n",
      "96                                            3238180          \n",
      "97                                            3238136          \n",
      "98                                            1498696          \n",
      "99                                            1498697          \n",
      "\n",
      "    transaction-aggregates-demo.max_transaction_amount_1m  \\\n",
      "0                                             1500172       \n",
      "1                                             3239672       \n",
      "2                                             1500166       \n",
      "3                                             1500185       \n",
      "4                                             1500193       \n",
      "..                                                ...       \n",
      "95                                            3239659       \n",
      "96                                            3239680       \n",
      "97                                            3239636       \n",
      "98                                            1500196       \n",
      "99                                            1500197       \n",
      "\n",
      "    transaction-aggregates-demo.count_transaction_amount_1h  \\\n",
      "0                                                3600         \n",
      "1                                                3600         \n",
      "2                                                3600         \n",
      "3                                                3600         \n",
      "4                                                3600         \n",
      "..                                                ...         \n",
      "95                                               3600         \n",
      "96                                               3600         \n",
      "97                                               3600         \n",
      "98                                               3600         \n",
      "99                                               3600         \n",
      "\n",
      "    transaction-aggregates-demo.sum_transaction_amount_1h  \\\n",
      "0                                          5076709200       \n",
      "1                                         11338909200       \n",
      "2                                          5076687600       \n",
      "3                                          5076756000       \n",
      "4                                          5076784800       \n",
      "..                                                ...       \n",
      "95                                        11338862400       \n",
      "96                                        11338938000       \n",
      "97                                        11338779600       \n",
      "98                                         5076795600       \n",
      "99                                         5076799200       \n",
      "\n",
      "    transaction-aggregates-demo.sample_stdev_transaction_amount_1h  \\\n",
      "0                                        51968.740604                \n",
      "1                                        51968.740604                \n",
      "2                                        51968.740604                \n",
      "3                                        51968.740604                \n",
      "4                                        51968.740604                \n",
      "..                                                ...                \n",
      "95                                       51968.740603                \n",
      "96                                       51968.740604                \n",
      "97                                       51968.740604                \n",
      "98                                       51968.740604                \n",
      "99                                       51968.740604                \n",
      "\n",
      "    transaction-aggregates-demo.median_transaction_amount_1h  \\\n",
      "0                                             1410172          \n",
      "1                                             3149672          \n",
      "2                                             1410166          \n",
      "3                                             1410185          \n",
      "4                                             1410193          \n",
      "..                                                ...          \n",
      "95                                            3149659          \n",
      "96                                            3149680          \n",
      "97                                            3149636          \n",
      "98                                            1410196          \n",
      "99                                            1410197          \n",
      "\n",
      "    transaction-aggregates-demo.max_transaction_amount_1h  \n",
      "0                                             1500172      \n",
      "1                                             3239672      \n",
      "2                                             1500166      \n",
      "3                                             1500185      \n",
      "4                                             1500193      \n",
      "..                                                ...      \n",
      "95                                            3239659      \n",
      "96                                            3239680      \n",
      "97                                            3239636      \n",
      "98                                            1500196      \n",
      "99                                            1500197      \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "from qwak.feature_store.offline import OfflineClientV2\n",
    "from qwak.feature_store.offline.feature_set_features import FeatureSetFeatures\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "offline_feature_store = OfflineClientV2()\n",
    "\n",
    "streaming_features = FeatureSetFeatures(\n",
    "            feature_set_name='transaction-aggregates-demo',\n",
    "            feature_names=['count_transaction_amount_1m',\n",
    "                        'last_distinct_5_transaction_amount_1m',\n",
    "                        'sum_transaction_amount_1m',\n",
    "                        'sample_stdev_transaction_amount_1m',\n",
    "                        'median_transaction_amount_1m',\n",
    "                        'max_transaction_amount_1m',\n",
    "                        'count_transaction_amount_1h',\n",
    "                        'sum_transaction_amount_1h',\n",
    "                        'sample_stdev_transaction_amount_1h',\n",
    "                        'median_transaction_amount_1h',\n",
    "                        'max_transaction_amount_1h'\n",
    "                        ]\n",
    "        )\n",
    "\n",
    "# Provide a Dataframe of entity id's and timestamps within the feature set ingestion \n",
    "# user_id | timestamp | label\n",
    "\n",
    "population_df = pd.read_csv(\"main/population.csv\")\n",
    "\n",
    "\n",
    "features: pd.DataFrame = offline_feature_store.get_feature_values(\n",
    "            features=[streaming_features],\n",
    "            population=population_df\n",
    "        )\n",
    "\n",
    "print(features)   \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f1cdd-b92d-4e74-a32a-361f7cea99ce",
   "metadata": {},
   "source": [
    "### Joining Features\n",
    "\n",
    "You can also join feature sets when retrieving from the feature store, including joining streaming and batch feature sets. As long as the feature sets share a common key/entity, you can pull features from both sets in the same query. Imagine we have a batch feature set of user credit features  such as job, credit_amount, duration, etc. Check out the example below to join streaming and batch feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b6494b-d4eb-4db4-b7f4-67a35f267507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>qwak-snowflake-webinar.job</th>\n",
       "      <th>qwak-snowflake-webinar.credit_amount</th>\n",
       "      <th>qwak-snowflake-webinar.duration</th>\n",
       "      <th>qwak-snowflake-webinar.purpose</th>\n",
       "      <th>qwak-snowflake-webinar.risk</th>\n",
       "      <th>transaction-aggregates-demo.count_transaction_amount_1m</th>\n",
       "      <th>transaction-aggregates-demo.last_distinct_5_transaction_amount_1m</th>\n",
       "      <th>transaction-aggregates-demo.sum_transaction_amount_1m</th>\n",
       "      <th>transaction-aggregates-demo.sample_stdev_transaction_amount_1m</th>\n",
       "      <th>transaction-aggregates-demo.median_transaction_amount_1m</th>\n",
       "      <th>transaction-aggregates-demo.max_transaction_amount_1m</th>\n",
       "      <th>transaction-aggregates-demo.count_transaction_amount_1h</th>\n",
       "      <th>transaction-aggregates-demo.sum_transaction_amount_1h</th>\n",
       "      <th>transaction-aggregates-demo.sample_stdev_transaction_amount_1h</th>\n",
       "      <th>transaction-aggregates-demo.median_transaction_amount_1h</th>\n",
       "      <th>transaction-aggregates-demo.max_transaction_amount_1h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-27 15:21:32.000</td>\n",
       "      <td>5f97899e-de8a-4527-b48b-633ec77dc722</td>\n",
       "      <td>1</td>\n",
       "      <td>3590</td>\n",
       "      <td>12</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>good</td>\n",
       "      <td>60</td>\n",
       "      <td>[1500164, 1500114, 1500064, 1500014, 1499964]</td>\n",
       "      <td>89921340</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>1498664</td>\n",
       "      <td>1500164</td>\n",
       "      <td>3600</td>\n",
       "      <td>5076680400</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>1410164</td>\n",
       "      <td>1500164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-27 15:21:32.000</td>\n",
       "      <td>742f09a6-b88b-4518-aa7b-431aa8ae8a16</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>18</td>\n",
       "      <td>education</td>\n",
       "      <td>bad</td>\n",
       "      <td>60</td>\n",
       "      <td>[1500166, 1500116, 1500066, 1500016, 1499966]</td>\n",
       "      <td>89921460</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>1498666</td>\n",
       "      <td>1500166</td>\n",
       "      <td>3600</td>\n",
       "      <td>5076687600</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>1410166</td>\n",
       "      <td>1500166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-27 15:21:32.000</td>\n",
       "      <td>d5eef17c-b25b-4c47-b29c-f02a5f1bd0ef</td>\n",
       "      <td>1</td>\n",
       "      <td>10613</td>\n",
       "      <td>21</td>\n",
       "      <td>car</td>\n",
       "      <td>good</td>\n",
       "      <td>60</td>\n",
       "      <td>[1500179, 1500129, 1500079, 1500029, 1499979]</td>\n",
       "      <td>89922240</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>1498679</td>\n",
       "      <td>1500179</td>\n",
       "      <td>3600</td>\n",
       "      <td>5076734400</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>1410179</td>\n",
       "      <td>1500179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-20 15:21:32.000</td>\n",
       "      <td>5f97899e-de8a-4527-b48b-633ec77dc722</td>\n",
       "      <td>1</td>\n",
       "      <td>3590</td>\n",
       "      <td>12</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>good</td>\n",
       "      <td>60</td>\n",
       "      <td>[3239664, 3239614, 3239564, 3239514, 3239464]</td>\n",
       "      <td>194291340</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>3238164</td>\n",
       "      <td>3239664</td>\n",
       "      <td>3600</td>\n",
       "      <td>11338880400</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>3149664</td>\n",
       "      <td>3239664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-20 15:21:32.000</td>\n",
       "      <td>742f09a6-b88b-4518-aa7b-431aa8ae8a16</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>18</td>\n",
       "      <td>education</td>\n",
       "      <td>bad</td>\n",
       "      <td>60</td>\n",
       "      <td>[3239666, 3239616, 3239566, 3239516, 3239466]</td>\n",
       "      <td>194291460</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>3238166</td>\n",
       "      <td>3239666</td>\n",
       "      <td>3600</td>\n",
       "      <td>11338887600</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>3149666</td>\n",
       "      <td>3239666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-27 15:21:32.000</td>\n",
       "      <td>73142300-493b-45f5-b355-3dadd54b0c13</td>\n",
       "      <td>3</td>\n",
       "      <td>2994</td>\n",
       "      <td>39</td>\n",
       "      <td>education</td>\n",
       "      <td>bad</td>\n",
       "      <td>60</td>\n",
       "      <td>[1500191, 1500141, 1500091, 1500041, 1499991]</td>\n",
       "      <td>89922960</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>1498691</td>\n",
       "      <td>1500191</td>\n",
       "      <td>3600</td>\n",
       "      <td>5076777600</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>1410191</td>\n",
       "      <td>1500191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-03-20 15:21:32.000</td>\n",
       "      <td>a3a2eaf9-5d8f-46dd-9cf4-9d186c973200</td>\n",
       "      <td>1</td>\n",
       "      <td>9883</td>\n",
       "      <td>41</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>good</td>\n",
       "      <td>60</td>\n",
       "      <td>[3239656, 3239606, 3239556, 3239506, 3239456]</td>\n",
       "      <td>194290860</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>3238156</td>\n",
       "      <td>3239656</td>\n",
       "      <td>3600</td>\n",
       "      <td>11338851600</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>3149656</td>\n",
       "      <td>3239656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-20 15:21:32.000</td>\n",
       "      <td>5d1d338c-4ac9-447b-b388-823951ede8f6</td>\n",
       "      <td>2</td>\n",
       "      <td>13194</td>\n",
       "      <td>11</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>bad</td>\n",
       "      <td>60</td>\n",
       "      <td>[3239684, 3239634, 3239584, 3239534, 3239484]</td>\n",
       "      <td>194292540</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>3238184</td>\n",
       "      <td>3239684</td>\n",
       "      <td>3600</td>\n",
       "      <td>11338952400</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>3149684</td>\n",
       "      <td>3239684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-20 15:21:32.000</td>\n",
       "      <td>181153df-581b-4cf9-9649-aee2eedb25d5</td>\n",
       "      <td>1</td>\n",
       "      <td>5813</td>\n",
       "      <td>20</td>\n",
       "      <td>car</td>\n",
       "      <td>bad</td>\n",
       "      <td>60</td>\n",
       "      <td>[3239640, 3239590, 3239540, 3239490, 3239440]</td>\n",
       "      <td>194289900</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>3238140</td>\n",
       "      <td>3239640</td>\n",
       "      <td>3600</td>\n",
       "      <td>11338794000</td>\n",
       "      <td>51968.740604</td>\n",
       "      <td>3149640</td>\n",
       "      <td>3239640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-20 15:21:32.000</td>\n",
       "      <td>73142300-493b-45f5-b355-3dadd54b0c13</td>\n",
       "      <td>3</td>\n",
       "      <td>2994</td>\n",
       "      <td>39</td>\n",
       "      <td>education</td>\n",
       "      <td>bad</td>\n",
       "      <td>60</td>\n",
       "      <td>[3239641, 3239591, 3239541, 3239491, 3239441]</td>\n",
       "      <td>194289960</td>\n",
       "      <td>873.21246</td>\n",
       "      <td>3238141</td>\n",
       "      <td>3239641</td>\n",
       "      <td>3600</td>\n",
       "      <td>11338797600</td>\n",
       "      <td>51968.740603</td>\n",
       "      <td>3149641</td>\n",
       "      <td>3239641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                timestamp                               user_id  \\\n",
       "0       0  2024-03-27 15:21:32.000  5f97899e-de8a-4527-b48b-633ec77dc722   \n",
       "1       0  2024-03-27 15:21:32.000  742f09a6-b88b-4518-aa7b-431aa8ae8a16   \n",
       "2       0  2024-03-27 15:21:32.000  d5eef17c-b25b-4c47-b29c-f02a5f1bd0ef   \n",
       "3       0  2024-03-20 15:21:32.000  5f97899e-de8a-4527-b48b-633ec77dc722   \n",
       "4       0  2024-03-20 15:21:32.000  742f09a6-b88b-4518-aa7b-431aa8ae8a16   \n",
       "..    ...                      ...                                   ...   \n",
       "95      0  2024-03-27 15:21:32.000  73142300-493b-45f5-b355-3dadd54b0c13   \n",
       "96      1  2024-03-20 15:21:32.000  a3a2eaf9-5d8f-46dd-9cf4-9d186c973200   \n",
       "97      0  2024-03-20 15:21:32.000  5d1d338c-4ac9-447b-b388-823951ede8f6   \n",
       "98      0  2024-03-20 15:21:32.000  181153df-581b-4cf9-9649-aee2eedb25d5   \n",
       "99      0  2024-03-20 15:21:32.000  73142300-493b-45f5-b355-3dadd54b0c13   \n",
       "\n",
       "    qwak-snowflake-webinar.job  qwak-snowflake-webinar.credit_amount  \\\n",
       "0                            1                                  3590   \n",
       "1                            0                                   750   \n",
       "2                            1                                 10613   \n",
       "3                            1                                  3590   \n",
       "4                            0                                   750   \n",
       "..                         ...                                   ...   \n",
       "95                           3                                  2994   \n",
       "96                           1                                  9883   \n",
       "97                           2                                 13194   \n",
       "98                           1                                  5813   \n",
       "99                           3                                  2994   \n",
       "\n",
       "    qwak-snowflake-webinar.duration qwak-snowflake-webinar.purpose  \\\n",
       "0                                12            furniture/equipment   \n",
       "1                                18                      education   \n",
       "2                                21                            car   \n",
       "3                                12            furniture/equipment   \n",
       "4                                18                      education   \n",
       "..                              ...                            ...   \n",
       "95                               39                      education   \n",
       "96                               41                       radio/TV   \n",
       "97                               11                       radio/TV   \n",
       "98                               20                            car   \n",
       "99                               39                      education   \n",
       "\n",
       "   qwak-snowflake-webinar.risk  \\\n",
       "0                         good   \n",
       "1                          bad   \n",
       "2                         good   \n",
       "3                         good   \n",
       "4                          bad   \n",
       "..                         ...   \n",
       "95                         bad   \n",
       "96                        good   \n",
       "97                         bad   \n",
       "98                         bad   \n",
       "99                         bad   \n",
       "\n",
       "    transaction-aggregates-demo.count_transaction_amount_1m  \\\n",
       "0                                                  60         \n",
       "1                                                  60         \n",
       "2                                                  60         \n",
       "3                                                  60         \n",
       "4                                                  60         \n",
       "..                                                ...         \n",
       "95                                                 60         \n",
       "96                                                 60         \n",
       "97                                                 60         \n",
       "98                                                 60         \n",
       "99                                                 60         \n",
       "\n",
       "   transaction-aggregates-demo.last_distinct_5_transaction_amount_1m  \\\n",
       "0       [1500164, 1500114, 1500064, 1500014, 1499964]                  \n",
       "1       [1500166, 1500116, 1500066, 1500016, 1499966]                  \n",
       "2       [1500179, 1500129, 1500079, 1500029, 1499979]                  \n",
       "3       [3239664, 3239614, 3239564, 3239514, 3239464]                  \n",
       "4       [3239666, 3239616, 3239566, 3239516, 3239466]                  \n",
       "..                                                ...                  \n",
       "95      [1500191, 1500141, 1500091, 1500041, 1499991]                  \n",
       "96      [3239656, 3239606, 3239556, 3239506, 3239456]                  \n",
       "97      [3239684, 3239634, 3239584, 3239534, 3239484]                  \n",
       "98      [3239640, 3239590, 3239540, 3239490, 3239440]                  \n",
       "99      [3239641, 3239591, 3239541, 3239491, 3239441]                  \n",
       "\n",
       "    transaction-aggregates-demo.sum_transaction_amount_1m  \\\n",
       "0                                            89921340       \n",
       "1                                            89921460       \n",
       "2                                            89922240       \n",
       "3                                           194291340       \n",
       "4                                           194291460       \n",
       "..                                                ...       \n",
       "95                                           89922960       \n",
       "96                                          194290860       \n",
       "97                                          194292540       \n",
       "98                                          194289900       \n",
       "99                                          194289960       \n",
       "\n",
       "    transaction-aggregates-demo.sample_stdev_transaction_amount_1m  \\\n",
       "0                                           873.21246                \n",
       "1                                           873.21246                \n",
       "2                                           873.21246                \n",
       "3                                           873.21246                \n",
       "4                                           873.21246                \n",
       "..                                                ...                \n",
       "95                                          873.21246                \n",
       "96                                          873.21246                \n",
       "97                                          873.21246                \n",
       "98                                          873.21246                \n",
       "99                                          873.21246                \n",
       "\n",
       "    transaction-aggregates-demo.median_transaction_amount_1m  \\\n",
       "0                                             1498664          \n",
       "1                                             1498666          \n",
       "2                                             1498679          \n",
       "3                                             3238164          \n",
       "4                                             3238166          \n",
       "..                                                ...          \n",
       "95                                            1498691          \n",
       "96                                            3238156          \n",
       "97                                            3238184          \n",
       "98                                            3238140          \n",
       "99                                            3238141          \n",
       "\n",
       "    transaction-aggregates-demo.max_transaction_amount_1m  \\\n",
       "0                                             1500164       \n",
       "1                                             1500166       \n",
       "2                                             1500179       \n",
       "3                                             3239664       \n",
       "4                                             3239666       \n",
       "..                                                ...       \n",
       "95                                            1500191       \n",
       "96                                            3239656       \n",
       "97                                            3239684       \n",
       "98                                            3239640       \n",
       "99                                            3239641       \n",
       "\n",
       "    transaction-aggregates-demo.count_transaction_amount_1h  \\\n",
       "0                                                3600         \n",
       "1                                                3600         \n",
       "2                                                3600         \n",
       "3                                                3600         \n",
       "4                                                3600         \n",
       "..                                                ...         \n",
       "95                                               3600         \n",
       "96                                               3600         \n",
       "97                                               3600         \n",
       "98                                               3600         \n",
       "99                                               3600         \n",
       "\n",
       "    transaction-aggregates-demo.sum_transaction_amount_1h  \\\n",
       "0                                          5076680400       \n",
       "1                                          5076687600       \n",
       "2                                          5076734400       \n",
       "3                                         11338880400       \n",
       "4                                         11338887600       \n",
       "..                                                ...       \n",
       "95                                         5076777600       \n",
       "96                                        11338851600       \n",
       "97                                        11338952400       \n",
       "98                                        11338794000       \n",
       "99                                        11338797600       \n",
       "\n",
       "    transaction-aggregates-demo.sample_stdev_transaction_amount_1h  \\\n",
       "0                                        51968.740604                \n",
       "1                                        51968.740604                \n",
       "2                                        51968.740604                \n",
       "3                                        51968.740604                \n",
       "4                                        51968.740604                \n",
       "..                                                ...                \n",
       "95                                       51968.740604                \n",
       "96                                       51968.740604                \n",
       "97                                       51968.740604                \n",
       "98                                       51968.740604                \n",
       "99                                       51968.740603                \n",
       "\n",
       "    transaction-aggregates-demo.median_transaction_amount_1h  \\\n",
       "0                                             1410164          \n",
       "1                                             1410166          \n",
       "2                                             1410179          \n",
       "3                                             3149664          \n",
       "4                                             3149666          \n",
       "..                                                ...          \n",
       "95                                            1410191          \n",
       "96                                            3149656          \n",
       "97                                            3149684          \n",
       "98                                            3149640          \n",
       "99                                            3149641          \n",
       "\n",
       "    transaction-aggregates-demo.max_transaction_amount_1h  \n",
       "0                                             1500164      \n",
       "1                                             1500166      \n",
       "2                                             1500179      \n",
       "3                                             3239664      \n",
       "4                                             3239666      \n",
       "..                                                ...      \n",
       "95                                            1500191      \n",
       "96                                            3239656      \n",
       "97                                            3239684      \n",
       "98                                            3239640      \n",
       "99                                            3239641      \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qwak.feature_store.offline import OfflineClientV2\n",
    "from qwak.feature_store.offline.feature_set_features import FeatureSetFeatures\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "offline_feature_store = OfflineClientV2()\n",
    "population_df = pd.read_csv(\"population.csv\")\n",
    "\n",
    "streaming_features = FeatureSetFeatures(\n",
    "    feature_set_name='transaction-aggregates-demo',\n",
    "    feature_names=['count_transaction_amount_1m',\n",
    "                'last_distinct_5_transaction_amount_1m',\n",
    "                'sum_transaction_amount_1m',\n",
    "                'sample_stdev_transaction_amount_1m',\n",
    "                'median_transaction_amount_1m',\n",
    "                'max_transaction_amount_1m',\n",
    "                'count_transaction_amount_1h',\n",
    "                'sum_transaction_amount_1h',\n",
    "                'sample_stdev_transaction_amount_1h',\n",
    "                'median_transaction_amount_1h',\n",
    "                'max_transaction_amount_1h'\n",
    "                ]\n",
    ")\n",
    "\n",
    "batch_features = FeatureSetFeatures(\n",
    "    feature_set_name='qwak-snowflake-webinar',\n",
    "    feature_names=['job','credit_amount','duration','purpose','risk']\n",
    ")\n",
    "features = [streaming_features, batch_features]\n",
    "offline_feature_store.get_feature_values(\n",
    "    features=features,\n",
    "    population=population_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233382a-91f0-4728-bac8-4e3a160f589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Consuming Streaming Features from the Online Feature Store using the Online Client\n",
    "\n",
    "Similar to the OfflineClient, the OnlineClient allows you to directly query the streaming aggregated features being stored in the Online Store\n",
    "\n",
    "Like we queried in the previous step, we'll pass in a list of entity id's that will be used to retrieve the values from the online feature store. Because the Online Store functions as a key/value lookup, we won't need to pass in a timestamp as there is only one value stored per entity in the Online Feature Store\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0bd55-dfbf-4dd8-82f8-eb5de466d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from qwak.feature_store.online.client import OnlineClient\n",
    "from qwak.model.schema_entities import FeatureStoreInput\n",
    "from qwak.model.schema import ModelSchema, InferenceOutput, FeatureStoreInput, Entity\n",
    "\n",
    "FEATURE_SET = 'transaction-aggregates-demo'\n",
    "\n",
    "model_schema = ModelSchema(\n",
    "            inputs=[\n",
    "                FeatureStoreInput(name=f'{FEATURE_SET}.count_transaction_amount_1m'),\n",
    "                FeatureStoreInput(name=f'{FEATURE_SET}.last_distinct_5_transaction_amount_1m'),\n",
    "                FeatureStoreInput(name=f'{FEATURE_SET}.sum_transaction_amount_1m'),\n",
    "                FeatureStoreInput(name=f'{FEATURE_SET}.sample_stdev_transaction_amount_1m'),\n",
    "                FeatureStoreInput(name=f'{FEATURE_SET}.median_transaction_amount_1m')\n",
    "            ],\n",
    "            outputs=[InferenceOutput(name=\"credit_score\", type=float)]\n",
    "        )\n",
    "    \n",
    "online_client = OnlineClient()\n",
    "\n",
    "df = pd.DataFrame(columns=['user',],\n",
    "                  data   =[['e41160de-0a56-47cf-8193-a0c97fe2e752'],\n",
    "                           ['b0ca3ac4-5432-4c21-8251-a6ae0d3ad874'],\n",
    "                           ['4b7af572-b249-4bae-9815-10ed3a2cd01d']])\n",
    "                  \n",
    "online_features = online_client.get_feature_values(model_schema, df)\n",
    "\n",
    "\n",
    "print(f\"\\n\\Realtime features extracted:\\n\\n{online_features.to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d0c67-859a-4148-933b-602c9ea0e783",
   "metadata": {},
   "source": [
    "###  Enriching Inference Requests with Features from Online Store\n",
    "\n",
    "Qwak also natively integrates the Model runtime with the Feature Store, offering an easy way to leverage very low-latency feature retrieval. This is done without specifically running a query, just by sending the feature set key in the model request input. This will automatically extract the latest features for that `key`, in our case `user_id` during a model serving request.\n",
    "\n",
    "Below is a sample model end to end CreditRisk Model that utilizes both the OfflineClient for retrieving training data, and the OnlineClient for retrieving online data for inference. In the `predict()` method, you'll notice the `extract_features` flag set to True. Once this flat is enabled, Qwak will natively integrate the Online Feature Store, pulling features based on the entity key, `user_id`, provided in the input requests. \n",
    "\n",
    "You can also find a reference to this model in the credit_risk_streaming model in the examples repository - https://github.com/qwak-ai/qwak-examples/blob/feature_store_examples/credit_risk_streaming/main/model.py. Check out the full example with enviornment file and deployment steps to build and deploy this model as a real-time endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca5ca2-e704-4a65-80f9-69f152484266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qwak.feature_store.offline import OfflineClientV2\n",
    "from qwak.feature_store.offline.feature_set_features import FeatureSetFeatures\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import qwak\n",
    "from qwak.model.base import QwakModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import pandas as pd\n",
    "from qwak.model.tools import run_local\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "RUNNING_FILE_ABSOLUTE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "\n",
    "class StreamingRiskModel(QwakModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params = {\n",
    "            'iterations': 100,\n",
    "            'learning_rate': 0.1,\n",
    "            'eval_metric': 'Accuracy',\n",
    "            'logging_level': 'Silent',\n",
    "            'use_best_model': True\n",
    "        }\n",
    "        self.catboost = CatBoostClassifier(**self.params)\n",
    "        self.metrics = {\n",
    "            'accuracy': 90,\n",
    "            'random_state': 42,\n",
    "            'test_size': .25\n",
    "        }\n",
    "        qwak.log_param(self.params)\n",
    "\n",
    "\n",
    "    def fetch_features(self):\n",
    "        \"\"\"\n",
    "        Read data from the offline feature store\n",
    "        :return: Feature Store DF\n",
    "        \"\"\"\n",
    "        print(\"Fetching data from the feature store\")\n",
    "        offline_feature_store = OfflineClientV2()\n",
    "        population_df = pd.read_csv(f\"{RUNNING_FILE_ABSOLUTE_PATH}/population.csv\")\n",
    "\n",
    "        streaming_features = FeatureSetFeatures(\n",
    "            feature_set_name='transaction-aggregates-demo',\n",
    "            feature_names=['count_transaction_amount_1m',\n",
    "                        'last_distinct_5_transaction_amount_1m',\n",
    "                        'sum_transaction_amount_1m',\n",
    "                        'sample_stdev_transaction_amount_1m',\n",
    "                        'median_transaction_amount_1m',\n",
    "                        'max_transaction_amount_1m',\n",
    "                        'count_transaction_amount_1h',\n",
    "                        'sum_transaction_amount_1h',\n",
    "                        'sample_stdev_transaction_amount_1h',\n",
    "                        'median_transaction_amount_1h',\n",
    "                        'max_transaction_amount_1h'\n",
    "                        ]\n",
    "        )\n",
    "\n",
    "        batch_features = FeatureSetFeatures(\n",
    "            feature_set_name='qwak-snowflake-webinar',\n",
    "            feature_names=['job','credit_amount','duration','purpose','risk']\n",
    "        )\n",
    "        features = [streaming_features, batch_features]\n",
    "        return offline_feature_store.get_feature_values(\n",
    "            features=features,\n",
    "            population=population_df\n",
    "        )\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build the Qwak model:\n",
    "            1. Fetch the feature values from the feature store\n",
    "            2. Train a naive Catboost model\n",
    "        \"\"\"\n",
    "        df = self.fetch_features()\n",
    "        print(df.columns)\n",
    "        train_df = df[[\"qwak-snowflake-webinar.job\", \"qwak-snowflake-webinar.credit_amount\", \"qwak-snowflake-webinar.duration\", \"qwak-snowflake-webinar.purpose\",\"transaction-aggregates-demo.count_transaction_amount_1m\",\"transaction-aggregates-demo.sum_transaction_amount_1m\",\"transaction-aggregates-demo.sample_stdev_transaction_amount_1m\",\"transaction-aggregates-demo.median_transaction_amount_1m\",\"transaction-aggregates-demo.max_transaction_amount_1m\",\"transaction-aggregates-demo.count_transaction_amount_1h\",\"transaction-aggregates-demo.sum_transaction_amount_1h\",\"transaction-aggregates-demo.sample_stdev_transaction_amount_1h\",\"transaction-aggregates-demo.median_transaction_amount_1h\",\"transaction-aggregates-demo.max_transaction_amount_1h\" ]]\n",
    "\n",
    "        y = df[\"qwak-snowflake-webinar.risk\"].map({'good':1,'bad':0})\n",
    "\n",
    "\n",
    "        categorical_features_indices = np.where(train_df.dtypes != np.float64)[0]\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(train_df, y, test_size=0.25, random_state=42)\n",
    "\n",
    "        train_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\n",
    "        validate_pool = Pool(X_validation, y_validation, cat_features=categorical_features_indices)\n",
    "\n",
    "        print(\"Fitting catboost model\")\n",
    "        self.catboost.fit(train_pool, eval_set=validate_pool)\n",
    "\n",
    "        y_predicted = self.catboost.predict(X_validation)\n",
    "        f1 = f1_score(y_validation, y_predicted)\n",
    "        \n",
    "        qwak.log_metric({'f1_score': f1})\n",
    "        qwak.log_metric({'iterations': self.params['iterations']})\n",
    "        qwak.log_metric({'learning_rate': self.params['learning_rate']})\n",
    "        qwak.log_metric({'accuracy': self.metrics['accuracy']})\n",
    "        qwak.log_metric({'random_state': self.metrics['random_state']})\n",
    "        qwak.log_metric({'test_size': self.metrics['test_size']})\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def schema(self):\n",
    "        from qwak.model.schema import ModelSchema, InferenceOutput, FeatureStoreInput, Entity\n",
    "        user_id = Entity(name=\"user_id\", type=str)\n",
    "        model_schema = ModelSchema(\n",
    "            entities=[user_id],\n",
    "            inputs=[\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.count_transaction_amount_1m\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.sum_transaction_amount_1m\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.sample_stdev_transaction_amount_1m\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.median_transaction_amount_1m\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.max_transaction_amount_1m\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.count_transaction_amount_1h\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.sum_transaction_amount_1h\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.sample_stdev_transaction_amount_1h\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.median_transaction_amount_1h\"),\n",
    "                FeatureStoreInput(entity=user_id, name=\"transaction-aggregates-demo.max_transaction_amount_1h\"),\n",
    "                FeatureStoreInput(entity=user_id, name='qwak-snowflake-webinar.job'),\n",
    "                FeatureStoreInput(entity=user_id, name='qwak-snowflake-webinar.credit_amount'),\n",
    "                FeatureStoreInput(entity=user_id, name='qwak-snowflake-webinar.duration'),\n",
    "                FeatureStoreInput(entity=user_id, name='qwak-snowflake-webinar.purpose'),\n",
    "\n",
    "            ],\n",
    "            outputs=[\n",
    "                InferenceOutput(name=\"Risk\", type=float)\n",
    "            ])\n",
    "        return model_schema\n",
    "\n",
    "    @qwak.api(feature_extraction=True)\n",
    "    def predict(self, df, extracted_df):\n",
    "        #### {\"user_id\": \"xxxx-xxx-xxx-xxxx\"}\n",
    "        return pd.DataFrame(self.catboost.predict(extracted_df[[\"qwak-snowflake-webinar.job\", \"qwak-snowflake-webinar.credit_amount\", \"qwak-snowflake-webinar.duration\", \"qwak-snowflake-webinar.purpose\",\"transaction-aggregates-demo.count_transaction_amount_1m\",\"transaction-aggregates-demo.sum_transaction_amount_1m\",\"transaction-aggregates-demo.sample_stdev_transaction_amount_1m\",\"transaction-aggregates-demo.median_transaction_amount_1m\",\"transaction-aggregates-demo.max_transaction_amount_1m\",\"transaction-aggregates-demo.count_transaction_amount_1h\",\"transaction-aggregates-demo.sum_transaction_amount_1h\",\"transaction-aggregates-demo.sample_stdev_transaction_amount_1h\",\"transaction-aggregates-demo.median_transaction_amount_1h\",\"transaction-aggregates-demo.max_transaction_amount_1h\" ]]),\n",
    "                            columns=['Risk'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
