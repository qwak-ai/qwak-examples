{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09416862-25e8-470f-bf85-01d287143bfb",
   "metadata": {},
   "source": [
    "# Qwak Feature Store Guide - Stream Feature Set with Pandas Transformations and Window Aggregations\n",
    "\n",
    "Welcome to the Qwak Feature Store example! In this tutorial, we'll guide you through creating a sample Data Source, transforming it into a Feature Set, and leveraging its features for model training and inference using the Qwak Platform. \n",
    "\n",
    "Guides like this one aim to provide you with a starting point by offering a straightforward framework for working with Qwak. However, we encourage you to explore our [comprehensive documentation](https://docs-saas.qwak.com/docs/feature-store-overview) for more detailed and specific information.\n",
    "\n",
    "Before diving in, make sure you have the Qwak SDK installed and authenticated. If you haven't done so already, follow these steps:\n",
    "\n",
    "1. [Install the Qwak SDK](https://docs-saas.qwak.com/docs/installing-the-qwak-sdk) - Ensure you have the SDK installed on your local environment.\n",
    "2. [Authenticate](https://docs-saas.qwak.com/docs/installing-the-qwak-sdk#1-via-qwak-cli) - Authenticate with a new Personal or Service Qwak API Key.\n",
    "\n",
    "To gain a deeper understanding of Feature Stores and their importance in machine learning workflows, we recommend checking out our comprehensive [documentation](https://docs-saas.qwak.com/docs/feature-store-overview) and our blog article on [What is a Feature Store](https://www.qwak.com/post/what-is-a-feature-store-in-ml). Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd037e5-c8b4-4c8d-9bdd-ce426dc22b09",
   "metadata": {},
   "source": [
    "## Create the Kafka-based Data Source\n",
    "\n",
    "In Qwak, a Data Source serves as a configuration object that specifies how to access and fetch your data. It includes metadata such as name and description, connection details to the data store/storage, the query or resource to retrieve, and the relevant time column for indexing time series features.\n",
    "\n",
    "In the following example, we'll connect to a publicly accessible Kafka broker hosted on AWS via MSK which contains the data from our Churn Model example.\n",
    "\n",
    "**Kafka Source supports authentication through user/password, so we will store those first in Qwak Secrets before creating the Data Source.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbee8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating secret named 'sample-kafka-user' with value length of 10...\n",
      "Created!\n",
      "Creating secret named 'sample-kafka-password' with value length of 14...\n",
      "Created!\n"
     ]
    }
   ],
   "source": [
    "!qwak secrets set --name 'sample-kafka-user' --value 'kafka-user'\n",
    "!qwak secrets set --name 'sample-kafka-password' --value 'kafka-password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff9c12dd-f46e-49e2-be07-931da11cf465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streaming_data_source.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streaming_data_source.py\n",
    "from qwak.feature_store.data_sources import *\n",
    "import json\n",
    "\n",
    "BOOTSTRAP_SERVERS = 'b-1-public.basicmskcluster.6o8ynq.c2.kafka.us-east-1.amazonaws.com:9196,b-2-public.basicmskcluster.6o8ynq.c2.kafka.us-east-1.amazonaws.com:9196' # comma-separated sequence of host:port entries, example: b-1-public.basicmskcluster.6o8ynq.c2.kafka.us-east-1.amazonaws.com:9196\n",
    "KAFKA_TOPIC = 'users' # comma separated list of 1 or more topics\n",
    "\n",
    "json_schema = {\n",
    "        \"type\": \"struct\",\n",
    "        \"fields\": [\n",
    "            {\"metadata\": {}, \"name\": \"churn\", \"nullable\": True, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"User_Id\", \"nullable\": False, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"State\", \"nullable\": True, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"Account_Length\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"Area_Code\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"Phone\", \"nullable\": True, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"Intl_Plan\", \"nullable\": True, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"VMail_Plan\", \"nullable\": True, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"VMail_Message\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"Day_Mins\", \"nullable\": True, \"type\": \"float\"},\n",
    "            {\"metadata\": {}, \"name\": \"Day_Calls\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"Eve_Mins\", \"nullable\": True, \"type\": \"float\"},\n",
    "            {\"metadata\": {}, \"name\": \"Eve_Calls\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"Night_Mins\", \"nullable\": True, \"type\": \"float\"},\n",
    "            {\"metadata\": {}, \"name\": \"Night_Calls\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"Intl_Mins\", \"nullable\": True, \"type\": \"float\"},\n",
    "            {\"metadata\": {}, \"name\": \"Intl_Calls\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"CustServ_Calls\", \"nullable\": True, \"type\": \"integer\"},\n",
    "            {\"metadata\": {}, \"name\": \"event date\", \"nullable\": True, \"type\": \"string\"},\n",
    "            {\"metadata\": {}, \"name\": \"Agitation_Level\", \"nullable\": True, \"type\": \"string\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Create deserializer with JSON schema\n",
    "deserializer = GenericDeserializer(\n",
    "\tmessage_format = MessageFormat.JSON,\n",
    "\tschema = json.dumps(json_schema)\n",
    ")\n",
    "\n",
    "kafka_source = KafkaSource(\n",
    "    name ='Risk_Data_Streaming',\n",
    "    description ='Risk Model Data Streamed from Kafka',       \n",
    "    bootstrap_servers = BOOTSTRAP_SERVERS,                      # List of HOST:PORT entries separated by comma\n",
    "    subscribe = KAFKA_TOPIC,                                    # List of Kafka topic/s to read from\n",
    "\t\tauthentication_method = SaslAuthentication(\n",
    "            username_secret = 'sample-kafka-user',              # Qwak Secret where the Kafka SASL/SCRAM user is stored\n",
    "            password_secret = 'sample-kafka-password',          # Qwak Secret where the Kafka SASL/SCRAM password is stored\n",
    "            sasl_mechanism = SaslMechanism.SCRAMSHA512,         # Qwak support authentication via SASL_SSL with SCRAM with various SHA options\n",
    "            security_protocol = SecurityProtocol.SASL_SSL,      # Qwak support authentication via SASL_SSL or Plaintext\n",
    "        ),\n",
    "\tdeserialization=deserializer,                               # JSON or AVRO deserializers are currently supported, together with Custom Deserializers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2484ec-e719-46d1-9d20-ef72e6f20747",
   "metadata": {},
   "source": [
    "### Additional Considerations for Registering Data Sources\n",
    "\n",
    "When registering Data Sources in Qwak, it's essential to ensure that the underlying data store is accessible by the platform. Depending on your deployment model (Hybrid or SaaS), there are different ways to grant Qwak access to your data.\n",
    "\n",
    "#### Accessing AWS Resources:\n",
    "\n",
    "If your data is stored in AWS services, you can grant access to Qwak using an IAM role ARN. For detailed instructions, refer to our documentation on [Accessing AWS Resources with IAM Role](https://docs-saas.qwak.com/docs/accessing-aws-resources-with-iam-role).\n",
    "\n",
    "#### Using Qwak Secrets:\n",
    "\n",
    "You can pass the Kafka SASL credentials as Qwak Secrets. This approach provides a secure way to manage and authenticate access to your data. For more information, see [Qwak Secret Management](https://docs-saas.qwak.com/docs/secret-management).\n",
    "\n",
    "For more information about the types of Data Sources supported by Qwak, refer to our documentation:\n",
    "- [Batch Data Sources](https://docs-saas.qwak.com/docs/batch-data-sources)\n",
    "- [Streaming Data Sources](https://docs-saas.qwak.com/docs/streaming-data-sources)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sampling Data from the Data Source\n",
    "\n",
    "It's important to note that the data source object cannot be used as a query engine independently (for now). However, it can serve as a sampling mechanism to verify that the data is being fetched properly before registration to the Qwak Platform.\n",
    "\n",
    "**The Kafka offset will be stored internally in the Qwak Data Engine and will read all the data from the topic.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4fcc6d5-50db-401f-a41e-4c1c8c8938ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source Data Types:\n",
      "\n",
      "churn               object\n",
      "User_Id             object\n",
      "State               object\n",
      "Account_Length       int64\n",
      "Area_Code            int64\n",
      "Phone               object\n",
      "Intl_Plan           object\n",
      "VMail_Plan          object\n",
      "VMail_Message        int64\n",
      "Day_Mins           float64\n",
      "Day_Calls            int64\n",
      "Eve_Mins           float64\n",
      "Eve_Calls            int64\n",
      "Night_Mins         float64\n",
      "Night_Calls          int64\n",
      "Intl_Mins          float64\n",
      "Intl_Calls           int64\n",
      "CustServ_Calls       int64\n",
      "event date          object\n",
      "Agitation_Level     object\n",
      "dtype: object\n",
      "\n",
      "Data Source Sample :\n",
      "\n",
      "  churn                               User_Id State  Account_Length  Area_Code     Phone Intl_Plan VMail_Plan  VMail_Message    Day_Mins  Day_Calls    Eve_Mins  Eve_Calls  Night_Mins  Night_Calls  Intl_Mins  Intl_Calls  CustServ_Calls           event date Agitation_Level\n",
      "0     0  ed377768-cab6-433d-af34-88b693c72d67    ND              59        510  351-4226         0          0              0  179.399994         80  232.500000         99  175.800003          105       14.7           3               0  2020-01-01 00:00:00              55\n",
      "1     0  d6e6de2d-3a4d-4795-82f7-317373eb9a33    NC             165        415  330-6630         0          0              0  207.699997        109  164.800003         94   54.500000           91        7.9           3               0  2020-01-01 00:00:00             135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run streaming_data_source.py\n",
    "\n",
    "df_sample = kafka_source.get_sample(2)\n",
    "print (f\"Data Source Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"Data Source Sample :\\n\\n{df_sample.head(7).to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d27c1-25fa-45d5-836c-d22aa3e35fab",
   "metadata": {},
   "source": [
    "## Registering the Data Source with the Qwak Platform\n",
    "\n",
    "After verifying that the Data Source returns the desired results, the next step is to register it with the Qwak Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4027c415-473c-4d62-a384-957d1f6dd7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Entities to register (0:00:00.00)\n",
      "ðŸ‘€ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "ðŸ‘€ Found 1 Data Sources\n",
      "Validating 'Risk_Data_Streaming' data source\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m  (0:00:04.98)\n",
      "âœ… Validation completed successfully, got data source columns:\n",
      "column name      type\n",
      "---------------  ------\n",
      "churn            string\n",
      "User_Id          string\n",
      "State            string\n",
      "Account_Length   int\n",
      "Area_Code        int\n",
      "Phone            string\n",
      "Intl_Plan        string\n",
      "VMail_Plan       string\n",
      "VMail_Message    int\n",
      "Day_Mins         float\n",
      "Day_Calls        int\n",
      "Eve_Mins         float\n",
      "Eve_Calls        int\n",
      "Night_Mins       float\n",
      "Night_Calls      int\n",
      "Intl_Mins        float\n",
      "Intl_Calls       int\n",
      "CustServ_Calls   int\n",
      "event date       string\n",
      "Agitation_Level  string\n",
      "Update existing Data Source 'Risk_Data_Streaming' from source file '/Users/haha/Projects/qwak-examples/feature_store/streaming_data_source.py'?\n",
      "continue? [y/N]: ----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "ðŸ‘€ Found 0 Feature Set(s)\n"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | qwak features register -p streaming_data_source.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fabf0-06a1-4429-9129-1a14d83c9fc8",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Creating the Streaming Feature Set from the Data Source\n",
    "\n",
    "When creating a Feature Set, it typically consists of the following components:\n",
    "\n",
    "- **Metadata:** Includes name, key, data sources, and the timestamp column used for indexing.\n",
    "- **Scheduling Expression:** For Batch Feature Sets, this defines when ingestion jobs should run.\n",
    "- **Cluster Type:** Specifies the resources to use for running the ingestion job.\n",
    "- **Backfill:** Determines how far back in time the Feature Set should ingest data.\n",
    "- **Transformation:** Can be SQL-based or UDF-based (currently Koalas) for data transformation.\n",
    "\n",
    "[Read Policies](https://docs-saas.qwak.com/docs/read-policies) instruct Qwak on which data to fetch from the Data Source. \n",
    "- **NewOnly:** Fetches records created after the last ingestion.\n",
    "- **TimeFrame:** Fetches records within a specified timeframe.\n",
    "- **FullRead:** Fetches all data from the Data Source in every ingestion job, which can be heavy for main tables but useful for foreign key-based tables.\n",
    "\n",
    "For this example, we'll use FullRead since our sample Data Source is static, consisting of a single CSV file.\n",
    "\n",
    "The execution specification refers to the size of the cluster used for data ingestion. More information can be found in the [Qwak docs](https://docs-saas.qwak.com/docs/instance-sizes#feature-store).\n",
    "\n",
    "### The feature engineering process\n",
    "\n",
    "As you're aware, a Koalas DataFrame is akin to a pandas DataFrame but operates across a Spark cluster. Once your data resides in a Koalas dataframe, it's advisable to carry out transformations directly within Koalas rather than switching to other data types like pandas. This is because switching to other dataframes, such as pandas, consolidates the distributed data into a single node, leading to sequential computation rather than parallel.\n",
    "\n",
    "The qwak `transform` method should return a KoalasTransformation object, indicating your end-to-end transformation function, in this case, `extract_features`. You're not restricted to a single function; you can divide the logic into as many as necessary and call them within a main function passed to the transformation.\n",
    "\n",
    "The function's input will be a Python dictionary with data source names as keys and their associated values as Koalas DataFrames containing the ingested data from the DataSource.\n",
    "\n",
    "It's important to note that while this code is callable locally, via `get_sample()`` as demonstrated in the next cell, it's not available for debugging, as the actual transformations will be executed remotely.\n",
    "\n",
    "To utilize Spark Pandas Transformation effectively, **ensure you have Python 3.8 installed** and `cloudpickle` locked to version `2.2.1`, as your code will be pickled and provided to Qwak for registration and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c201c7e-3b50-4c14-80d2-75d46734169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streaming_agg_feature_set.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streaming_agg_feature_set.py\n",
    "\n",
    "import pandas as pd\n",
    "from qwak.feature_store.feature_sets import streaming\n",
    "from qwak.feature_store.feature_sets.transformations import (\n",
    "    Column,\n",
    "    Schema,\n",
    "    QwakAggregation,\n",
    "    SparkSqlTransformation,\n",
    "    Type,\n",
    "    qwak_pandas_udf\n",
    ")\n",
    "from qwak.feature_store.feature_sets.execution_spec import ClusterTemplate\n",
    "\n",
    "@qwak_pandas_udf(output_schema=Schema([Column(type=Type.timestamp)]))\n",
    "def parse_date(event_dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(event_dates)\n",
    "\n",
    "@streaming.feature_set(\n",
    "    name=\"telecom-aggregated-usage\",\n",
    "    key=\"user_id\",\n",
    "    data_sources=[\"Risk_Data_Streaming\"],\n",
    "    timestamp_column_name=\"timestamp\",\n",
    ")\n",
    "@streaming.execution_specification(\n",
    "    online_cluster_template=ClusterTemplate.NANO,\n",
    "    offline_cluster_template=ClusterTemplate.NANO,\n",
    ")\n",
    "\n",
    "def transform():\n",
    "    sql = \"\"\"\n",
    "        \n",
    "        SELECT\n",
    "            User_Id,\n",
    "            Day_Mins,\n",
    "            Day_Calls,\n",
    "            Eve_Calls,\n",
    "            Night_Calls,\n",
    "            CustServ_Calls,\n",
    "            Agitation_Level,\n",
    "            \n",
    "            -- Derived Features\n",
    "            (Day_Calls + Eve_Calls + Night_Calls) AS Total_Calls,\n",
    "            \n",
    "            -- Date Column\n",
    "            parse_date(`event date`) as timestamp,\n",
    "\n",
    "            -- Required for the window aggregations\n",
    "            topic,\n",
    "            partition,\n",
    "            offset\n",
    "\n",
    "        FROM Risk_Data_Streaming\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        SparkSqlTransformation( sql,\n",
    "                                functions=[parse_date])\n",
    "        .aggregate(QwakAggregation.avg(\"Day_Mins\"))\n",
    "        .aggregate(QwakAggregation.avg(\"CustServ_Calls\"))\n",
    "        .aggregate(QwakAggregation.max(\"Agitation_Level\"))\n",
    "        .aggregate(QwakAggregation.sum(\"Total_Calls\"))\n",
    "        .by_windows(\"7 days\", \"30 days\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea960ab-ffb2-4dd9-997d-494829ff625f",
   "metadata": {},
   "source": [
    "## Sampling the Data Source and Printing Data and Data Types\n",
    "\n",
    "If your data source takes more than 5 minutes to query or fetch a sample of the data (for example, due to long-running queries), your sampling process may fail with a timeout error. In such cases, you can skip validation during registration with Qwak and proceed to register your feature set, allowing it to run an ingestion job.\n",
    "\n",
    "### Note:\n",
    "The sampling process is essential for verifying that the data is queried properly. However, if it takes too long, you can proceed with the registration without validation and rely on the ingestion job to ensure data correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75631eb-b157-47e3-9a5e-c65960acfd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source Data Types:\n",
      "\n",
      "user_id                     object\n",
      "avg_CustServ_Calls_7d      float64\n",
      "sum_Total_Calls_7d           int64\n",
      "avg_Day_Mins_7d            float64\n",
      "max_Agitation_Level_7d      object\n",
      "avg_CustServ_Calls_30d     float64\n",
      "sum_Total_Calls_30d          int64\n",
      "avg_Day_Mins_30d           float64\n",
      "max_Agitation_Level_30d     object\n",
      "dtype: object\n",
      "\n",
      "Data Source Sample :\n",
      "\n",
      "                                user_id  avg_CustServ_Calls_7d  sum_Total_Calls_7d  avg_Day_Mins_7d max_Agitation_Level_7d  avg_CustServ_Calls_30d  sum_Total_Calls_30d  avg_Day_Mins_30d max_Agitation_Level_30d\n",
      "0  d6e6de2d-3a4d-4795-82f7-317373eb9a33                    0.0                 294       207.699997                    135                     0.0                  294        207.699997                     135\n",
      "1  2f370912-53c0-4cce-81ce-ea803f30207b                    2.0                 299       192.600006                    108                     2.0                  299        192.600006                     108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run streaming_agg_feature_set.py\n",
    "\n",
    "df_sample = transform.get_sample()\n",
    "print (f\"Window Aggregations Feature Set Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"Window Aggregations Feature Set Sample :\\n\\n{df_sample.head(2).to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9868350",
   "metadata": {},
   "source": [
    "### Creating a feature set with Pandas UDFs and SQL, without aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00be4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streaming_feature_set.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streaming_feature_set.py\n",
    "\n",
    "import pandas as pd\n",
    "from qwak.feature_store.feature_sets import streaming\n",
    "from qwak.feature_store.feature_sets.transformations import (\n",
    "    Column,\n",
    "    Schema,\n",
    "    SparkSqlTransformation,\n",
    "    Type,\n",
    "    qwak_pandas_udf\n",
    ")\n",
    "from qwak.feature_store.feature_sets.execution_spec import ClusterTemplate\n",
    "\n",
    "@qwak_pandas_udf(output_schema=Schema([Column(type=Type.timestamp)]))\n",
    "def parse_date(event_dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(event_dates)\n",
    "\n",
    "@streaming.feature_set(\n",
    "    name=\"telecom-usage-features\",\n",
    "    key=\"user_id\",\n",
    "    data_sources=[\"Risk_Data_Streaming\"],\n",
    "    timestamp_column_name=\"timestamp\",\n",
    "    offline_scheduling_policy=\"*/5 * * * *\",\n",
    "    online_trigger_interval=30\n",
    ")\n",
    "@streaming.execution_specification(\n",
    "    online_cluster_template=ClusterTemplate.NANO,\n",
    "    offline_cluster_template=ClusterTemplate.NANO,\n",
    ")\n",
    "\n",
    "def transform():\n",
    "    sql = \"\"\"\n",
    "        \n",
    "        SELECT\n",
    "            User_Id,\n",
    "            Day_Mins,\n",
    "            Day_Calls,\n",
    "            Eve_Calls,\n",
    "            Night_Calls,\n",
    "            CustServ_Calls,\n",
    "            Agitation_Level,\n",
    "            \n",
    "            -- Derived Features\n",
    "            (Day_Mins + Eve_Mins + Night_Mins) AS Total_Mins,\n",
    "            (Day_Calls + Eve_Calls + Night_Calls) AS Total_Calls,\n",
    "            (Day_Mins + Eve_Mins + Night_Mins) / NULLIF((Day_Calls + Eve_Calls + Night_Calls), 0) AS Mins_Per_Call,\n",
    "            Intl_Mins / NULLIF((Day_Mins + Eve_Mins + Night_Mins), 0) AS Intl_Usage_Ratio,\n",
    "            CustServ_Calls / NULLIF((Day_Calls + Eve_Calls + Night_Calls), 0) AS CustServ_Call_Ratio,\n",
    "            \n",
    "            -- Date Column\n",
    "            parse_date(`event date`) as timestamp\n",
    "\n",
    "        FROM Risk_Data_Streaming\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        SparkSqlTransformation( sql,\n",
    "                                functions=[parse_date])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ce5f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Feature Set Data Types:\n",
      "\n",
      "User_Id                        object\n",
      "Day_Mins                      float64\n",
      "Day_Calls                       int64\n",
      "Eve_Calls                       int64\n",
      "Night_Calls                     int64\n",
      "CustServ_Calls                  int64\n",
      "Agitation_Level                object\n",
      "Total_Mins                    float64\n",
      "Total_Calls                     int64\n",
      "Mins_Per_Call                 float64\n",
      "Intl_Usage_Ratio              float64\n",
      "CustServ_Call_Ratio           float64\n",
      "timestamp              datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "SQL Feature Set Sample :\n",
      "\n",
      "                                User_Id    Day_Mins  Day_Calls  Eve_Calls  Night_Calls  CustServ_Calls Agitation_Level  Total_Mins  Total_Calls  Mins_Per_Call  Intl_Usage_Ratio  CustServ_Call_Ratio  timestamp\n",
      "0  8fbe0c29-b2fd-4772-946a-5b4eb0087648  197.199997        118         70          104               0             162       746.0          292       2.554795          0.005228             0.000000 2020-01-01\n",
      "1  2f370912-53c0-4cce-81ce-ea803f30207b  192.600006         97        101          101               2             108       544.0          299       1.819398          0.014522             0.006689 2020-01-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run streaming_feature_set.py\n",
    "\n",
    "df_sample = transform.get_sample()\n",
    "print (f\"SQL Feature Set Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"SQL Feature Set Sample :\\n\\n{df_sample.head(2).to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5f593-7e74-466e-8e9a-22cd614ba101",
   "metadata": {},
   "source": [
    "## Visualizing Data in the Feature Store\n",
    "\n",
    "The displayed data represents the features stored in the feature store, which will be utilized in our Qwak ML model for both training and inference purposes.\n",
    "\n",
    "Once we have confirmed that the data appears as expected and meets our requirements, we can proceed with registering the feature set in Qwak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7d7713c-bf02-4d33-bd85-4af4ba6d3544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice that BatchInferenceClient and FeedbackClient are not available in the skinny package. In order to use them, please install them as extras: pip install \"qwak-inference[batch,feedback]\".\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Entities to register (0:00:00.10)\n",
      "ðŸ‘€ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "ðŸ‘€ Found 0 Data Sources\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "ðŸ‘€ Found 1 Feature Set(s)\n",
      "Update existing feature set 'telecom-aggregated-usage' from source file '/Users/haha/Projects/qwak-examples/feature_store/streaming_agg_feature_set.py'?\n",
      "continue? [y/N]: Validating 'telecom-aggregated-usage' feature set\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m  (0:00:05.85)\n",
      "âœ… Validation completed successfully, got data source columns:\n",
      "column name              type\n",
      "-----------------------  ------\n",
      "user_id                  string\n",
      "avg_CustServ_Calls_7d    double\n",
      "sum_Total_Calls_7d       bigint\n",
      "avg_Day_Mins_7d          double\n",
      "max_Agitation_Level_7d   string\n",
      "avg_CustServ_Calls_30d   double\n",
      "sum_Total_Calls_30d      bigint\n",
      "avg_Day_Mins_30d         double\n",
      "max_Agitation_Level_30d  string\n",
      "\u001b[91mFailed to update feature set, error is Updating a Transformation object for streaming aggregation feature set is not allowed. Any addition should currently be handled in a different feature set.\u001b[0m\n",
      "Notice that BatchInferenceClient and FeedbackClient are not available in the skinny package. In order to use them, please install them as extras: pip install \"qwak-inference[batch,feedback]\".\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Entities to register (0:00:00.11)\n",
      "ðŸ‘€ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "ðŸ‘€ Found 0 Data Sources\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "ðŸ‘€ Found 1 Feature Set(s)\n",
      "Update existing feature set 'telecom-usage-features' from source file '/Users/haha/Projects/qwak-examples/feature_store/streaming_feature_set.py'?\n",
      "continue? [y/N]: Validating 'telecom-usage-features' feature set\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m  (0:00:05.29)\n",
      "âœ… Validation completed successfully, got data source columns:\n",
      "column name          type\n",
      "-------------------  ---------\n",
      "User_Id              string\n",
      "Day_Mins             float\n",
      "Day_Calls            int\n",
      "Eve_Calls            int\n",
      "Night_Calls          int\n",
      "CustServ_Calls       int\n",
      "Agitation_Level      string\n",
      "Total_Mins           float\n",
      "Total_Calls          int\n",
      "Mins_Per_Call        double\n",
      "Intl_Usage_Ratio     double\n",
      "CustServ_Call_Ratio  double\n",
      "timestamp            timestamp\n",
      "\u001b[91mFailed to update feature set, error is Can not update a Feature Set in [REGISTRATION_IN_PROGRESS] state\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | qwak features register -p streaming_agg_feature_set.py\n",
    "!echo \"Y\" | qwak features register -p streaming_feature_set.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239e7f1-dd0d-40b7-9250-be2596f80438",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Verifying Feature Set Registration\n",
    "\n",
    "To ensure that the Feature Set has been successfully registered and is valid, execute the following command to list all Feature Sets associated with your Qwak account:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d490ed-03c5-45d7-82c8-9a3d2bf80d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!qwak features list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd0aa9-3524-4abc-aecd-5b05f6f54886",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For more information on the available Feature Store SDK commands, please use the CLI help:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d091acf-289a-4c72-abc1-5f7d88e09e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice that BatchInferenceClient and FeedbackClient are not available in the skinny package. In order to use them, please install them as extras: pip install \"qwak-inference[batch,feedback]\".\n",
      "Usage: qwak features [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Commands for interacting with the Qwak Feature Store\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  backfill          Trigger a backfill process for a Feature Set\n",
      "  delete            Delete by name a feature store object - a feature...\n",
      "  execution-status  Retrieve the current status of an execution...\n",
      "  list              List registered feature sets\n",
      "  pause             Pause a running feature set\n",
      "  register          Register and deploy all feature store object under...\n",
      "  resume            Resume a paused feature set\n",
      "  trigger           Trigger a batch feature set job ingestion job\n"
     ]
    }
   ],
   "source": [
    "!qwak features --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe40f5-2638-4386-9772-cb2589c73bad",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Consuming Features from the Offline Feature Store (Training/Batch Inference)\n",
    "\n",
    "To retrieve features from the Offline Feature Store for training or batch inference, you can use two methods:\n",
    "\n",
    "1. **By List of IDs and Timestamp**:\n",
    "   - Fetches records associated with the provided set of keys, inserted at a specific timestamp.\n",
    "   - Query date must fall between the start and end timestamp.\n",
    "\n",
    "2. **By Date Range**:\n",
    "   - Retrieves all records within the specified date range.\n",
    "   - May include multiple records per key for time series data.\n",
    "\n",
    "\n",
    "For simplicity we will focus on the second option, but you can find more information on the first one in [our docs](https://docs-saas.qwak.com/docs/getting-features-for-training#get-feature-values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "484e6d82-bfdd-4256-bc7d-23580713f76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "QwakException",
     "evalue": "\u001b[91mGot the following run-time exception: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"There was a server error trying to handle an exception\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"There was a server error trying to handle an exception\", grpc_status:13, created_time:\"2024-07-29T12:40:44.101109+03:00\"}\"\n>\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/feature_store/offline/client_v2.py:100\u001b[0m, in \u001b[0;36mOfflineClientV2.get_feature_range_values\u001b[0;34m(self, features, start_date, end_date, population)\u001b[0m\n\u001b[1;32m     98\u001b[0m upper_time_bound_proto: ProtoTimestamp \u001b[38;5;241m=\u001b[39m datetime_to_pts(end_date)\n\u001b[1;32m     99\u001b[0m response: ProtoGetFeatureValuesResultResponse \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fs_offline_serving_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_values_in_range_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlower_time_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_time_bound_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupper_time_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_time_bound_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_file_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mProtoFileFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFILE_FORMAT_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpopulation_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OfflineClientV2\u001b[38;5;241m.\u001b[39m_results_response_to_df(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/clients/feature_store/offline_serving_client.py:203\u001b[0m, in \u001b[0;36mFsOfflineServingClient.get_feature_values_in_range_blocking\u001b[0;34m(self, features, lower_time_bound, upper_time_bound, result_file_format, population, timeout_seconds, poll_interval_seconds)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_values_in_range_blocking\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     features: ProtoFeaturesetFeatures,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     poll_interval_seconds: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    202\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ProtoGetFeatureValuesResultResponse:\n\u001b[0;32m--> 203\u001b[0m     request_handle: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_values_in_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlower_time_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_time_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupper_time_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_time_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_file_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_file_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_for_result(\n\u001b[1;32m    212\u001b[0m         request_handle\u001b[38;5;241m=\u001b[39mrequest_handle,\n\u001b[1;32m    213\u001b[0m         timeout_seconds\u001b[38;5;241m=\u001b[39mtimeout_seconds,\n\u001b[1;32m    214\u001b[0m         poll_interval_seconds\u001b[38;5;241m=\u001b[39mpoll_interval_seconds,\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/clients/feature_store/offline_serving_client.py:106\u001b[0m, in \u001b[0;36mFsOfflineServingClient.get_feature_values_in_range\u001b[0;34m(self, features, lower_time_bound, upper_time_bound, result_file_format, population)\u001b[0m\n\u001b[1;32m     95\u001b[0m request: ProtoGetFeatureValuesInRangeRequest \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     96\u001b[0m     ProtoGetFeatureValuesInRangeRequest(\n\u001b[1;32m     97\u001b[0m         features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m response: ProtoGetFeatureValuesInRangeResponse \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetFeatureValuesInRange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mrequest_id\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:439\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:1193\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1187\u001b[0m (\n\u001b[1;32m   1188\u001b[0m     state,\n\u001b[1;32m   1189\u001b[0m     call,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1191\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1192\u001b[0m )\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"There was a server error trying to handle an exception\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"There was a server error trying to handle an exception\", grpc_status:13, created_time:\"2024-07-29T12:40:44.101109+03:00\"}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mQwakException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m feature_range_start \u001b[38;5;241m=\u001b[39m datetime(year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2019\u001b[39m, month\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, day\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m feature_range_end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\n\u001b[0;32m---> 35\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_training_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_range_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_range_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining data sample:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrain_df\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39mto_string()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m, in \u001b[0;36mfetch_training_features\u001b[0;34m(start_time, end_time)\u001b[0m\n\u001b[1;32m     15\u001b[0m features \u001b[38;5;241m=\u001b[39m FeatureSetFeatures(\n\u001b[1;32m     16\u001b[0m     feature_set_name\u001b[38;5;241m=\u001b[39m FEATURE_SET,\n\u001b[1;32m     17\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39m FEATURES_LIST)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# It's recommended to be surrounded in a try/catch\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m features: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43moffline_feature_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_range_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_time\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/feature_store/offline/client_v2.py:115\u001b[0m, in \u001b[0;36mOfflineClientV2.get_feature_range_values\u001b[0;34m(self, features, start_date, end_date, population)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m QwakException(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot the following Qwak generated exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqwak_exception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m QwakException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot the following run-time exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mQwakException\u001b[0m: \u001b[91mGot the following run-time exception: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"There was a server error trying to handle an exception\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"There was a server error trying to handle an exception\", grpc_status:13, created_time:\"2024-07-29T12:40:44.101109+03:00\"}\"\n>\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Importing the Feature Store clients used to fetch results\n",
    "from qwak.feature_store.offline import OfflineClientV2\n",
    "from qwak.feature_store.offline.feature_set_features import FeatureSetFeatures\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "FEATURE_SET = 'telecom-usage-features'\n",
    "FEATURES_LIST = ['Day_Mins', 'Day_Calls', 'Eve_Calls', 'Night_Calls', 'CustServ_Calls', 'Agitation_Level', 'Total_Mins', 'Total_Calls', 'Mins_Per_Call', 'Intl_Usage_Ratio', 'CustServ_Call_Ratio']\n",
    "\n",
    "def fetch_training_features(start_time: datetime, end_time: datetime) -> pd.DataFrame: \n",
    "\n",
    "    offline_feature_store = OfflineClientV2()\n",
    "    \n",
    "    features = FeatureSetFeatures(\n",
    "        feature_set_name= FEATURE_SET,\n",
    "        feature_names= FEATURES_LIST)\n",
    "    \n",
    "    # It's recommended to be surrounded in a try/catch\n",
    "    features: pd.DataFrame = offline_feature_store.get_feature_range_values(\n",
    "        features=features,\n",
    "        start_date=start_time,\n",
    "        end_date=end_time\n",
    "    )\n",
    "\n",
    "    return features\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Define the date range for feature retrieval\n",
    "    feature_range_start = datetime(year=2019, month=12, day=1)\n",
    "    feature_range_end = datetime.today()\n",
    "\n",
    "    train_df = fetch_training_features(feature_range_start, feature_range_end)\n",
    "\n",
    "    print(f\"\\n\\nTraining data sample:\\n\\n{train_df.head().to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76ff96-59bf-4f5c-89d2-4450891cbeb5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Please note that although the Feature Set has been registered, it usually takes a couple of minutes to run the first ingestion job. This means you might not have any data to fetch until the ingestion job runs at least once.\n",
    "\n",
    "To verify the status of the ingestion, please refer to the Qwak Dashboard -> Feature Sets -> `credit-risk-fs-sql` -> Jobs.\n",
    "\n",
    "![Feature Store Dashboard](ingestion-job-finished.png)\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c4ec0-68f9-4657-91bb-31096e686acb",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Consuming Features for Real-Time Inference from the Online Store\n",
    "\n",
    "In the previous example, we retrieved historical data from the Offline Store, which is storing all the historical data. Now, we'll use the Online Store, which is optimized for real-time use-cases and provides a low-latency feature retrieval mechanism. \n",
    "Qwak provides two ways to query the Online store and look up the most recent feature vector for a given key:\n",
    "\n",
    "###  1. Enriching Inference Requests with Features from Online Store\n",
    "\n",
    "Qwak natively integrates the Model runtime with the Feature Store, offering an easy way to leverage very low-latency feature retrieval. This is done without specifically running a query, just by sending the feature set key in the model request input. This will automatically extract the latest features for that `key`, in our case `user` during a model serving request.\n",
    "\n",
    "\n",
    "Note: Below is an example code for local use only. If you're using it for a live model, please remove the `run_local` import.\n",
    "\n",
    "**The ModelSchema definition is mandatory to enable feature extraction via the OnlineClient or qwak.api decorator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9228cbeb-db89-4410-b021-b7cc2d654e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.model.tools import run_local # utility tooling for local testing and debugging - REMOVE BEFORE BUILDING REMOTELY\n",
    "\n",
    "from qwak.model.base import QwakModel\n",
    "from qwak.model.adapters import DefaultOutputAdapter, DataFrameInputAdapter\n",
    "from qwak.model.schema import ModelSchema, InferenceOutput\n",
    "from qwak.model.schema_entities import FeatureStoreInput\n",
    "import pandas as pd\n",
    "import qwak\n",
    "\n",
    "class ChurnPredictionModel(QwakModel):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def schema(self) -> ModelSchema:\n",
    "        model_schema = ModelSchema(\n",
    "            inputs= [FeatureStoreInput(name=f'{FEATURE_SET}.{feature}') for feature in FEATURES_LIST],\n",
    "            outputs=[InferenceOutput(name=\"churn_probability\", type=float)]\n",
    "        )\n",
    "        return model_schema\n",
    "\n",
    "    @qwak.api(\n",
    "        feature_extraction=True,\n",
    "        input_adapter=DataFrameInputAdapter(),\n",
    "        output_adapter=DefaultOutputAdapter()\n",
    "    )\n",
    "    def predict(self, df: pd.DataFrame, extracted_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(f\"\\nInput dataframe df:\\n{df}\")\n",
    "        print(f\"\\nFeature Set extracted dataframe:\\n{extracted_df.to_string()}\")\n",
    "        return pd.DataFrame([['score', 0.5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedb207-6892-4f65-a61f-05ad7904b52e",
   "metadata": {},
   "source": [
    "Calling the model locally to test `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b15b602-fb16-4a28-aad6-161266975a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for: \n",
      "\n",
      " {\"user_id\":{\"0\":\"0166f628-07a6-4461-9870-fc9df0df7a5b\"}}\n"
     ]
    },
    {
     "ename": "QwakException",
     "evalue": "\u001b[91mFailed to get online features results. Error is: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"upstream connect error or disconnect/reset before headers. reset reason: protocol error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"upstream connect error or disconnect/reset before headers. reset reason: protocol error\", grpc_status:14, created_time:\"2024-07-29T13:54:17.95888+03:00\"}\"\n>\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/feature_store/online/client.py:200\u001b[0m, in \u001b[0;36mOnlineClient.get_feature_values\u001b[0;34m(self, schema, df, model_name)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     response_df_json, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetMultiFeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_value_requests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(response_df_json\u001b[38;5;241m.\u001b[39mpandas_df_as_json, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:343\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:439\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:1193\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1187\u001b[0m (\n\u001b[1;32m   1188\u001b[0m     state,\n\u001b[1;32m   1189\u001b[0m     call,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1191\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1192\u001b[0m )\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"upstream connect error or disconnect/reset before headers. reset reason: protocol error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"upstream connect error or disconnect/reset before headers. reset reason: protocol error\", grpc_status:14, created_time:\"2024-07-29T13:54:17.95888+03:00\"}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mQwakException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m run_local(m, json_payload)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtest_model_locally\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mtest_model_locally\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting for: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json_payload)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run local inference using the model and print the prediction\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# The run_local function is part of the qwak library and allows for local testing of the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mrun_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_payload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/model/tools/run_model_locally.py:48\u001b[0m, in \u001b[0;36mrun_local\u001b[0;34m(model, payload)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (_DF \u001b[38;5;129;01mor\u001b[39;00m _EXTRACTED_DF) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m            Missing 'extracted_df' or 'df' arguments in function invocation, even though 'feature_extraction' flag is true.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m            \"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         )\n\u001b[0;32m---> 48\u001b[0m     extracted_df \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_online_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     output_data \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(input_data, extracted_df)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/model/tools/run_model_locally.py:57\u001b[0m, in \u001b[0;36m_extract_online_features\u001b[0;34m(model, df)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_online_features\u001b[39m(model, df):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOnlineClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest Model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/feature_store/online/client.py:208\u001b[0m, in \u001b[0;36mOnlineClient.get_feature_values\u001b[0;34m(self, schema, df, model_name)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    205\u001b[0m         [df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), result_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m QwakException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get online features results. Error is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mQwakException\u001b[0m: \u001b[91mFailed to get online features results. Error is: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"upstream connect error or disconnect/reset before headers. reset reason: protocol error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"upstream connect error or disconnect/reset before headers. reset reason: protocol error\", grpc_status:14, created_time:\"2024-07-29T13:54:17.95888+03:00\"}\"\n>\u001b[0m"
     ]
    }
   ],
   "source": [
    "def test_model_locally():\n",
    "    # Create a new instance of the model\n",
    "    m = ChurnPredictionModel()\n",
    "\n",
    "    # Define the columns\n",
    "    columns = [\"user_id\"]\n",
    "\n",
    "    # Define the data\n",
    "    data = [[\"0166f628-07a6-4461-9870-fc9df0df7a5b\"]]\n",
    "    \n",
    "    # Create the DataFrame and convert it to JSON\n",
    "    json_payload = pd.DataFrame(data=data, columns=columns).to_json()\n",
    "\n",
    "    print(\"Predicting for: \\n\\n\", json_payload)\n",
    "    \n",
    "\n",
    "    # Run local inference using the model and print the prediction\n",
    "    # The run_local function is part of the qwak library and allows for local testing of the model\n",
    "    prediction = run_local(m, json_payload)\n",
    "    print(\"\\nPrediction: \", prediction)\n",
    "\n",
    "test_model_locally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed62ec8-3f5f-4be5-976c-597ed647a2cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "As you can see, the we only sent the `user` ID in the prediction request, and Qwak automatically extracted the relevant (latest) features for that key from the Feature Set specified in the Model Schema. \n",
    "\n",
    "This approach is automatically logging the extraction latency to the model Analytics.\n",
    "\n",
    "<br>\n",
    "\n",
    "###  2. Features Lookup with the OnlineClient\n",
    "\n",
    "With the previous approach we managed to enable a QwakModel to fetch features automatically and that approach is great for most cases. However what happens if we want to have more control over the keys we want to look up for at runtime, like for example looking up multiple keys for a single prediction request input. \n",
    "\n",
    "That's what the `OnlineClient` is for, to enable you explicit queries, as we'll exemplify below:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cf537b",
   "metadata": {},
   "outputs": [
    {
     "ename": "QwakException",
     "evalue": "\u001b[91mFailed to get online features results. Error is: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"upstream connect error or disconnect/reset before headers. reset reason: protocol error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"upstream connect error or disconnect/reset before headers. reset reason: protocol error\", grpc_status:14, created_time:\"2024-07-15T18:48:18.663342+03:00\"}\"\n>\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/feature_store/online/client.py:200\u001b[0m, in \u001b[0;36mOnlineClient.get_feature_values\u001b[0;34m(self, schema, df, model_name)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     response_df_json, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetMultiFeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_value_requests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(response_df_json\u001b[38;5;241m.\u001b[39mpandas_df_as_json, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:343\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:439\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:1193\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1187\u001b[0m (\n\u001b[1;32m   1188\u001b[0m     state,\n\u001b[1;32m   1189\u001b[0m     call,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1191\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1192\u001b[0m )\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"upstream connect error or disconnect/reset before headers. reset reason: protocol error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"upstream connect error or disconnect/reset before headers. reset reason: protocol error\", grpc_status:14, created_time:\"2024-07-15T18:48:18.663342+03:00\"}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mQwakException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m online_client \u001b[38;5;241m=\u001b[39m OnlineClient()\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m,],\n\u001b[1;32m     14\u001b[0m                   data   \u001b[38;5;241m=\u001b[39m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06cc255a-aa07-4ec9-ac69-b896ccf05322\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m                            [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m46ad9e4b-1d0f-47b7-a73d-71cc66538b03\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     16\u001b[0m                            [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m95ec0c53-4e27-4490-b85f-1448de70fc26\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m---> 18\u001b[0m online_features \u001b[38;5;241m=\u001b[39m \u001b[43monline_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRealtime features extracted:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00monline_features\u001b[38;5;241m.\u001b[39mto_string()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwak-new-3.8/lib/python3.8/site-packages/qwak/feature_store/online/client.py:208\u001b[0m, in \u001b[0;36mOnlineClient.get_feature_values\u001b[0;34m(self, schema, df, model_name)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    205\u001b[0m         [df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), result_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m QwakException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get online features results. Error is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mQwakException\u001b[0m: \u001b[91mFailed to get online features results. Error is: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"upstream connect error or disconnect/reset before headers. reset reason: protocol error\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"upstream connect error or disconnect/reset before headers. reset reason: protocol error\", grpc_status:14, created_time:\"2024-07-15T18:48:18.663342+03:00\"}\"\n>\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from qwak.feature_store.online.client import OnlineClient\n",
    "from qwak.model.schema_entities import FeatureStoreInput\n",
    "from qwak.model.schema import ModelSchema\n",
    "\n",
    "model_schema = ModelSchema(\n",
    "    inputs= [FeatureStoreInput(name=f'{FEATURE_SET}.{feature}') for feature in FEATURES_LIST],\n",
    "    outputs=[InferenceOutput(name=\"credit_score\", type=float)]\n",
    ")\n",
    "    \n",
    "online_client = OnlineClient()\n",
    "\n",
    "df = pd.DataFrame(columns=['user_id',],\n",
    "                  data   =[['06cc255a-aa07-4ec9-ac69-b896ccf05322'],\n",
    "                           ['46ad9e4b-1d0f-47b7-a73d-71cc66538b03'],\n",
    "                           ['95ec0c53-4e27-4490-b85f-1448de70fc26']])\n",
    "                  \n",
    "online_features = online_client.get_feature_values(model_schema, df)\n",
    "\n",
    "\n",
    "print(f\"\\n\\Realtime features extracted:\\n\\n{online_features.to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4cae4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You may have noticed that the FeatureStoreInput names contain both the feature set name and the feature name. This design allows you to specify and utilize multiple feature sets within the same request.\n",
    "\n",
    "Similar to the previous option, the `ModelSchema` is a required component. It informs Qwak about the features to include in the lookup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
