{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09416862-25e8-470f-bf85-01d287143bfb",
   "metadata": {},
   "source": [
    "# JFrogML Feature Store Example - Batch Feature Set with PySpark Transformation\n",
    "\n",
    "Welcome to the JFrogML Feature Store example! In this tutorial, we'll guide you through creating a sample Data Source, transforming it into a Feature Set, and leveraging its features for model training and inference using the JFrogML Platform. \n",
    "\n",
    "Guides like this one aim to provide you with a starting point by offering a straightforward framework for working with JFrogML. However, we encourage you to explore our [comprehensive documentation](https://docs.qwak.com/docs/feature-store-overview) for more detailed and specific information.\n",
    "\n",
    "Before diving in, make sure you have the JFrogML SDK installed and authenticated. If you haven't done so already, follow these steps:\n",
    "\n",
    "1. [Install the JFrogML SDK](https://docs.qwak.com/docs/transformations#pyspark) - Ensure you have the SDK installed on your local environment.\n",
    "2. [Authenticate](https://docs.qwak.com/docs/installing-the-qwak-sdk#1-via-qwak-cli) - Authenticate with a new Personal or Service Qwak API Key.\n",
    "\n",
    "To gain a deeper understanding of Feature Stores and their importance in machine learning workflows, we recommend checking out our comprehensive [documentation](https://docs.qwak.com/docs/feature-store-overview) and our blog article on [What is a Feature Store](https://www.qwak.com/post/what-is-a-feature-store-in-ml). Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd037e5-c8b4-4c8d-9bdd-ce426dc22b09",
   "metadata": {},
   "source": [
    "## Create the S3-based Data Source\n",
    "\n",
    "In JFrogML, a Data Source serves as a configuration object that specifies how to access and fetch your data. It includes metadata such as name and description, connection details to the data store/storage, the query or resource to retrieve, and the relevant time column for indexing time series features.\n",
    "\n",
    "### Components of a Data Source:\n",
    "\n",
    "1. **Metadata**: Includes information like name, description, etc.\n",
    "2. **URL and Connection Details**: Specifies the connection details to the data store/storage.\n",
    "3. **Query or Resource**: Defines the resource (file, table, view) to retrieve data from.\n",
    "4. **Time Column**: Specifies the relevant time column for indexing time series features.\n",
    "\n",
    "In the following example, we'll connect to a publicly accessible S3 bucket and fetch data from a single CSV file, for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9c12dd-f46e-49e2-be07-931da11cf465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_source.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_source.py\n",
    "\n",
    "from qwak.feature_store.data_sources import CsvSource\n",
    "import pandas as pd\n",
    "\n",
    "# The S3 anonymous config class is required for public S3 buckets\n",
    "from qwak.feature_store.data_sources import AnonymousS3Configuration\n",
    "\n",
    "# Create a CsvSource object to represent a CSV data source \n",
    "# This example uses a CSV file from a public S3 bucket\n",
    "csv_source = CsvSource(\n",
    "    name='credit_risk_data',                                    # Name of the data source\n",
    "    description='A dataset of personal credit details',         # Description of the data source\n",
    "    date_created_column='date_created',                         # Column name that represents the creation date\n",
    "    path='s3://qwak-public/example_data/data_credit_risk.csv',  # S3 path to the CSV file \n",
    "    filesystem_configuration=AnonymousS3Configuration(),        # Configuration for anonymous access to S3\n",
    "    quote_character='\"',                                        # Character used for quoting in the CSV file\n",
    "    escape_character='\"'                                        # Character used for escaping in the CSV file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2484ec-e719-46d1-9d20-ef72e6f20747",
   "metadata": {},
   "source": [
    "### Additional Considerations for Registering Data Sources\n",
    "\n",
    "When registering Data Sources in JFrogML, it's essential to ensure that the underlying data store is accessible by the platform. Depending on your deployment model (Hybrid or SaaS), there are different ways to grant JFrogML access to your data.\n",
    "\n",
    "#### Accessing AWS Resources:\n",
    "\n",
    "If your data is stored in AWS services, you can grant access to JFrogML using an IAM role ARN. For detailed instructions, refer to our documentation on [Accessing AWS Resources with IAM Role](https://docs.qwak.com/docs/accessing-aws-resources-with-iam-role).\n",
    "\n",
    "#### Using JFrogML Secrets:\n",
    "\n",
    "Alternatively, you can pass the credentials as JFrogML Secrets. This approach provides a secure way to manage and authenticate access to your data. For more information, see [JFrogML Secret Management](https://docs.qwak.com/docs/secret-management).\n",
    "\n",
    "For more information about the types of Data Sources supported by JFrogML, refer to our documentation:\n",
    "- [Batch Data Sources](https://docs.qwak.com/docs/batch-data-sources)\n",
    "- [Streaming Data Sources](https://docs.qwak.com/docs/streaming-data-sources)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sampling Data from the Data Source\n",
    "\n",
    "It's important to note that the data source cannot be used as a query engine independently (for now). Instead, it serves as a sampling mechanism to verify that the data is being queried properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fcc6d5-50db-401f-a41e-4c1c8c8938ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source Data Types:\n",
      "\n",
      "age                  int64\n",
      "sex                 object\n",
      "job                  int64\n",
      "housing             object\n",
      "saving_account      object\n",
      "checking_account    object\n",
      "credit_amount        int64\n",
      "duration             int64\n",
      "purpose             object\n",
      "risk                object\n",
      "user_id             object\n",
      "date_created         int64\n",
      "dtype: object\n",
      "\n",
      "Data Source Sample :\n",
      "\n",
      "   age     sex  job housing saving_account checking_account  credit_amount  duration              purpose  risk                               user_id   date_created\n",
      "0   67    male    2     own           None           little           1169         6             radio/TV  good  baf1aed9-b16a-46f1-803b-e2b08c8b47de  1609459200000\n",
      "1   22  female    2     own         little         moderate           5951        48             radio/TV   bad  574a2cb7-f3ae-48e7-bd32-b44015bf9dd4  1609459200000\n",
      "2   49    male    1     own         little             None           2096        12            education  good  1b044db3-3bd1-4b71-a4e9-336210d6503f  1609459200000\n",
      "3   45    male    2    free         little           little           7882        42  furniture/equipment  good  ac8ec869-1a05-4df9-9805-7866ca42b31c  1609459200000\n",
      "4   53    male    2    free         little           little           4870        24                  car   bad  aa974eeb-ed0e-450b-90d0-4fe4592081c1  1609459200000\n",
      "5   35    male    1    free           None             None           9055        36            education  good  7b3d019c-82a7-42d9-beb8-2c57a246ff16  1609459200000\n",
      "6   53    male    2     own     quite rich             None           2835        24  furniture/equipment  good  6bc1fd70-897e-49f4-ae25-960d490cb74e  1609459200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run data_source.py\n",
    "\n",
    "df_sample = csv_source.get_sample()\n",
    "print (f\"Data Source Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"Data Source Sample :\\n\\n{df_sample.head(7).to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d27c1-25fa-45d5-836c-d22aa3e35fab",
   "metadata": {},
   "source": [
    "## Registering the Data Source with the JFrogML Platform\n",
    "\n",
    "After verifying that the Data Source returns the desired results, the next step is to register it with the JFrogML Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4027c415-473c-4d62-a384-957d1f6dd7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice that BatchInferenceClient and FeedbackClient are not available in the skinny package. In order to use them, please install them as extras: pip install \"qwak-inference[batch,feedback]\".\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Entities to register (0:00:00.01)\n",
      "ðŸ‘€ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "ðŸ‘€ Found 1 Data Sources\n",
      "Validating 'credit_risk_data' data source\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m  (0:00:04.52)\n",
      "âœ… Validation completed successfully, got data source columns:\n",
      "column name       type\n",
      "----------------  ---------\n",
      "age               int\n",
      "sex               string\n",
      "job               int\n",
      "housing           string\n",
      "saving_account    string\n",
      "checking_account  string\n",
      "credit_amount     int\n",
      "duration          int\n",
      "purpose           string\n",
      "risk              string\n",
      "user_id           string\n",
      "date_created      timestamp\n",
      "Update existing Data Source 'credit_risk_data' from source file '/Users/haha/Projects/qwak-examples/feature_store/data_source.py'?\n",
      "continue? [y/N]: ----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "ðŸ‘€ Found 0 Feature Set(s)\n"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | qwak features register -p data_source.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fabf0-06a1-4429-9129-1a14d83c9fc8",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Creating the Batch Feature Set from the Data Source\n",
    "\n",
    "When creating a Feature Set, it typically consists of the following components:\n",
    "\n",
    "- **Metadata:** Includes name, key, data sources, and the timestamp column used for indexing.\n",
    "- **Scheduling Expression:** For Batch Feature Sets, this defines when ingestion jobs should run.\n",
    "- **Cluster Type:** Specifies the resources to use for running the ingestion job.\n",
    "- **Backfill:** Determines how far back in time the Feature Set should ingest data.\n",
    "- **Transformation:** Can be SQL-based, PySpark or UDF based via Spark Pandas for data transformation.\n",
    "\n",
    "[Read Policies](https://docs.qwak.com/docs/read-policies) instruct JFrogML on which data to fetch from the Data Source. \n",
    "- **NewOnly:** Fetches records created after the last ingestion.\n",
    "- **TimeFrame:** Fetches records within a specified timeframe.\n",
    "- **FullRead:** Fetches all data from the Data Source in every ingestion job, which can be heavy for main tables but useful for foreign key-based tables.\n",
    "\n",
    "For this example, we'll use FullRead since our sample Data Source is static, consisting of a single CSV file.\n",
    "\n",
    "The execution specification refers to the size of the cluster used for data ingestion. More information can be found in the [JFrogML docs](https://docs.qwak.com/docs/instance-sizes#feature-store).\n",
    "\n",
    "### The PySpark transformation process\n",
    "\n",
    "As you're aware, a PySpark DataFrame is akin to a pandas DataFrame but operates across a Spark cluster. Once your data resides in a PySpark dataframe, it's advisable to carry out transformations directly within PySpark rather than switching to other data types like pandas. This is because switching to other dataframes, such as pandas, consolidates the distributed data into a single node, leading to sequential computation rather than parallel.\n",
    "\n",
    "The qwak `transform` method should return a PySparkTransformation object, indicating your end-to-end transformation function, in this case, `extract_features`. You're not restricted to a single function; you can divide the logic into as many as necessary and call them within a main function passed to the transformation.\n",
    "\n",
    "The function's input will be a Python dictionary with data source names as keys and their associated values as PySpark DataFrames containing the ingested data from the DataSource.\n",
    "\n",
    "It's important to note that while this code is callable locally, via `get_sample()`` as demonstrated in the next cell, it's not available for debugging, as the actual transformations will be executed remotely.\n",
    "\n",
    "To utilize PySpark Transformation effectively, **ensure you have Python 3.8 installed** and `cloudpickle` locked to version `2.2.1`, as your code will be pickled and provided to JFrogML for registration and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49ac7938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_feature_set_pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_feature_set_pyspark.py\n",
    "\n",
    "from typing import Dict, Any\n",
    "import pyspark.sql as spark  \n",
    "import pyspark.sql.functions as F\n",
    "from qwak.feature_store.feature_sets import batch  \n",
    "from qwak.feature_store.feature_sets.read_policies import ReadPolicy  \n",
    "from qwak.feature_store.feature_sets.transformations import PySparkTransformation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Global function definitions\n",
    "# These functions are defined globally and can be reused across multiple transformations.\n",
    "# They will be available to any part of the code that imports or references them.\n",
    "# This is useful when you want to define utility functions that can be shared across multiple feature sets or logic.\n",
    "\n",
    "def preprocess_date(df: spark.DataFrame) -> spark.DataFrame:\n",
    "\n",
    "    df = df.withColumn('year', F.year(df['date_created']))\n",
    "    df = df.withColumn('month', F.month(df['date_created']))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_age_groups(df: spark.DataFrame) -> spark.DataFrame:\n",
    "    def age_group(age):\n",
    "        if age < 25:\n",
    "            return '18-25'\n",
    "        elif 25 <= age < 35:\n",
    "            return '26-35'\n",
    "        elif 35 <= age < 45:\n",
    "            return '36-45'\n",
    "        elif 45 <= age < 55:\n",
    "            return '46-55'\n",
    "        elif 55 <= age < 65:\n",
    "            return '56-65'\n",
    "        else:\n",
    "            return '65+'\n",
    "    \n",
    "    udf_age_group = F.udf(age_group)\n",
    "    df = df.withColumn('age_group', udf_age_group(df['age']))\n",
    "    return df\n",
    "\n",
    "@batch.feature_set(  \n",
    "    name=\"credit-risk-fs-pyspark\",  \n",
    "    key=\"user_id\",  \n",
    "    data_sources={\"credit_risk_data\": ReadPolicy.FullRead},  \n",
    "    timestamp_column_name=\"date_created\"  \n",
    ")  \n",
    "@batch.scheduling(cron_expression=\"0 0 * * *\")  \n",
    "def transform():\n",
    "\n",
    "    # Functions defined within the `transform` method\n",
    "    # These are local to the 'transform' function and only available within its scope.\n",
    "\n",
    "    def calculate_credit_duration_ratio(df: spark.DataFrame) -> spark.DataFrame:\n",
    "        df = df.withColumn('credit_duration_ratio', df['credit_amount'] / df['duration'])\n",
    "        return df\n",
    "\n",
    "    def aggregate_user_data(df: spark.DataFrame) -> spark.DataFrame:\n",
    "        agg_df = df.groupby('user_id').agg(\n",
    "            F.mean('credit_amount').alias('credit_amount_mean'),\n",
    "            F.sum('credit_amount').alias('credit_amount_sum'),\n",
    "            F.mean('duration').alias('duration_mean'),\n",
    "            F.sum('duration').alias('duration_sum'),\n",
    "            F.first('age').alias('age_first'),\n",
    "            F.first('job').alias('job_first'),\n",
    "            F.mean('risk').alias('risk_first')\n",
    "        )\n",
    "        return agg_df\n",
    "\n",
    "    def merge_aggregated_data(df: spark.DataFrame, agg_df: spark.DataFrame) -> spark.DataFrame:\n",
    "        result_df = df.join(agg_df, on='user_id', how='left')\n",
    "        result_df = result_df.withColumn('credit_amount_diff_from_avg', result_df['credit_amount'] - result_df['credit_amount_mean'])\n",
    "        result_df = result_df.withColumn('duration_diff_from_avg', result_df['duration'] - result_df['duration_mean'])\n",
    "        return result_df\n",
    "\n",
    "    def extract_features(df_dict: Dict[str, spark.DataFrame]) -> spark.DataFrame:\n",
    "        # Extract the PySpark DataFrame containing credit risk data\n",
    "        risk_data_spark = df_dict['credit_risk_data']\n",
    "        \n",
    "        # Apply transformations\n",
    "        risk_data_spark = preprocess_date(risk_data_spark)\n",
    "        #risk_data_spark = encode_categorical_variables(risk_data_spark)\n",
    "        risk_data_spark = create_age_groups(risk_data_spark)\n",
    "        risk_data_spark = calculate_credit_duration_ratio(risk_data_spark)\n",
    "        \n",
    "        # Aggregate and merge\n",
    "        agg_df = aggregate_user_data(risk_data_spark)\n",
    "        result_df = merge_aggregated_data(risk_data_spark, agg_df)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    return PySparkTransformation(function=extract_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea960ab-ffb2-4dd9-997d-494829ff625f",
   "metadata": {},
   "source": [
    "## Sampling the Data Source and Printing Data and Data Types\n",
    "\n",
    "If your data source takes more than 5 minutes to query or fetch a sample of the data (for example, due to long-running queries), your sampling process may fail with a timeout error. In such cases, you can skip validation during registration with Qwak and proceed to register your feature set, allowing it to run an ingestion job.\n",
    "\n",
    "### Note:\n",
    "The sampling process is essential for verifying that the data is queried properly. However, if it takes too long, you can proceed with the registration without validation and rely on the ingestion job to ensure data correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c75631eb-b157-47e3-9a5e-c65960acfd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source Data Types:\n",
      "\n",
      "user_id                         object\n",
      "age                              int64\n",
      "sex                             object\n",
      "job                              int64\n",
      "housing                         object\n",
      "saving_account                  object\n",
      "checking_account                object\n",
      "credit_amount                    int64\n",
      "duration                         int64\n",
      "purpose                         object\n",
      "risk                            object\n",
      "date_created                     int64\n",
      "year                             int64\n",
      "month                            int64\n",
      "age_group                       object\n",
      "credit_duration_ratio          float64\n",
      "credit_amount_mean             float64\n",
      "credit_amount_sum                int64\n",
      "duration_mean                  float64\n",
      "duration_sum                     int64\n",
      "age_first                        int64\n",
      "job_first                        int64\n",
      "risk_first                      object\n",
      "credit_amount_diff_from_avg    float64\n",
      "duration_diff_from_avg         float64\n",
      "dtype: object\n",
      "\n",
      "Data Source Sample :\n",
      "\n",
      "                                user_id  age     sex  job housing saving_account checking_account  credit_amount  duration   purpose  risk   date_created  year  month age_group  credit_duration_ratio  credit_amount_mean  credit_amount_sum  duration_mean  duration_sum  age_first  job_first risk_first  credit_amount_diff_from_avg  duration_diff_from_avg\n",
      "0  baf1aed9-b16a-46f1-803b-e2b08c8b47de   67    male    2     own           None           little           1169         6  radio/TV  good  1609459200000  2021      1       65+             194.833333              1169.0               1169            6.0             6         67          2       None                          0.0                     0.0\n",
      "1  574a2cb7-f3ae-48e7-bd32-b44015bf9dd4   22  female    2     own         little         moderate           5951        48  radio/TV   bad  1609459200000  2021      1     18-25             123.979167              5951.0               5951           48.0            48         22          2       None                          0.0                     0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run batch_feature_set_pyspark.py\n",
    "\n",
    "df_sample = transform.get_sample()\n",
    "print (f\"Data Source Data Types:\\n\\n{df_sample.dtypes}\\n\")\n",
    "print (f\"Data Source Sample :\\n\\n{df_sample.head(2).to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5f593-7e74-466e-8e9a-22cd614ba101",
   "metadata": {},
   "source": [
    "## Visualizing Data in the Feature Store\n",
    "\n",
    "The displayed data represents the features stored in the feature store, which will be utilized in our JFrogML model for both training and inference purposes.\n",
    "\n",
    "Once we have confirmed that the data appears as expected and meets our requirements, we can proceed with registering the feature set in JFrogML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d7713c-bf02-4d33-bd85-4af4ba6d3544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Entities to register (0:00:00.20)\n",
      "ðŸ‘€ Found 0 Entities\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Data Sources to register (0:00:00.00)\n",
      "ðŸ‘€ Found 0 Data Sources\n",
      "----------------------------------------\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m Finding Feature Sets to register (0:00:00.00)\n",
      "ðŸ‘€ Found 1 Feature Set(s)\n",
      "Create new feature set 'credit-risk-fs-pyspark' from source file '/Users/haha/Projects/qwak-examples/feature_store/batch_feature_set_pyspark.py'?\n",
      "continue? [y/N]: Validating 'credit-risk-fs-pyspark' feature set\n",
      "\u001b[K\u001b[?25h\u001b[34mâœ…\u001b[0m  (0:00:05.27)\n",
      "âœ… Validation completed successfully, got data source columns:\n",
      "column name                  type\n",
      "---------------------------  ---------\n",
      "user_id                      string\n",
      "age                          int\n",
      "sex                          string\n",
      "job                          int\n",
      "housing                      string\n",
      "saving_account               string\n",
      "checking_account             string\n",
      "credit_amount                int\n",
      "duration                     int\n",
      "purpose                      string\n",
      "risk                         string\n",
      "date_created                 timestamp\n",
      "year                         int\n",
      "month                        int\n",
      "age_group                    string\n",
      "credit_duration_ratio        double\n",
      "credit_amount_mean           double\n",
      "credit_amount_sum            bigint\n",
      "duration_mean                double\n",
      "duration_sum                 bigint\n",
      "age_first                    int\n",
      "job_first                    int\n",
      "risk_first                   double\n",
      "credit_amount_diff_from_avg  double\n",
      "duration_diff_from_avg       double\n"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | qwak features register -p batch_feature_set_pyspark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239e7f1-dd0d-40b7-9250-be2596f80438",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Verifying Feature Set Registration\n",
    "\n",
    "To ensure that the Feature Set has been successfully registered and is valid, execute the following command to list all Feature Sets associated with your JFrogML account:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d490ed-03c5-45d7-82c8-9a3d2bf80d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!qwak features list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd0aa9-3524-4abc-aecd-5b05f6f54886",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For more information on the available Feature Store SDK commands, please use the CLI help:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d091acf-289a-4c72-abc1-5f7d88e09e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice that BatchInferenceClient and FeedbackClient are not available in the skinny package. In order to use them, please install them as extras: pip install \"qwak-inference[batch,feedback]\".\n",
      "Usage: qwak features [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Commands for interacting with the Qwak Feature Store\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  backfill          Trigger a backfill process for a Feature Set\n",
      "  delete            Delete by name a feature store object - a feature...\n",
      "  execution-status  Retrieve the current status of an execution...\n",
      "  list              List registered feature sets\n",
      "  pause             Pause a running feature set\n",
      "  register          Register and deploy all feature store object under...\n",
      "  resume            Resume a paused feature set\n",
      "  trigger           Trigger a batch feature set job ingestion job\n"
     ]
    }
   ],
   "source": [
    "!qwak features --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe40f5-2638-4386-9772-cb2589c73bad",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Consuming Features from the Offline Feature Store (Training/Batch Inference)\n",
    "\n",
    "To retrieve features from the Offline Feature Store for training or batch inference, you can use two methods:\n",
    "\n",
    "1. **By List of IDs and Timestamp**:\n",
    "   - Fetches records associated with the provided set of keys, inserted at a specific timestamp.\n",
    "   - Query date must fall between the start and end timestamp.\n",
    "\n",
    "2. **By Date Range**:\n",
    "   - Retrieves all records within the specified date range.\n",
    "   - May include multiple records per key for time series data.\n",
    "\n",
    "\n",
    "For simplicity we will focus on the second option, but you can find more information on the first one in [our docs](https://docs.qwak.com/docs/getting-features-for-training#get-feature-values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "484e6d82-bfdd-4256-bc7d-23580713f76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data sample:\n",
      "\n",
      "                                user_id                    date_created  credit-risk-fs-pyspark.age  credit-risk-fs-pyspark.job  credit-risk-fs-pyspark.credit_amount  credit-risk-fs-pyspark.duration  credit-risk-fs-pyspark.year  credit-risk-fs-pyspark.month credit-risk-fs-pyspark.sex credit-risk-fs-pyspark.housing credit-risk-fs-pyspark.saving_account credit-risk-fs-pyspark.checking_account credit-risk-fs-pyspark.purpose credit-risk-fs-pyspark.risk credit-risk-fs-pyspark.age_group  credit-risk-fs-pyspark.credit_duration_ratio  credit-risk-fs-pyspark.credit_amount_mean  credit-risk-fs-pyspark.credit_amount_sum  credit-risk-fs-pyspark.duration_mean  credit-risk-fs-pyspark.duration_sum  credit-risk-fs-pyspark.age_first  credit-risk-fs-pyspark.job_first  credit-risk-fs-pyspark.risk_first  credit-risk-fs-pyspark.credit_amount_diff_from_avg  credit-risk-fs-pyspark.duration_diff_from_avg\n",
      "0  45b7836f-bf7c-4039-bc9e-d33982cc1fc5  2023-03-20 23:00:00.000000 UTC                          27                           2                                  4576                               45                         2023                             3                       male                            own                              moderate                                moderate                            car                        good                            26-35                                    101.688889                                     4576.0                                      9152                                  45.0                                   90                                27                                 2                                NaN                                                 0.0                                            0.0\n",
      "1  45b7836f-bf7c-4039-bc9e-d33982cc1fc5  2021-01-01 00:00:00.000000 UTC                          27                           2                                  4576                               45                         2021                             1                       male                            own                              moderate                                moderate                            car                        good                            26-35                                    101.688889                                     4576.0                                      9152                                  45.0                                   90                                27                                 2                                NaN                                                 0.0                                            0.0\n",
      "2  35343bfd-f15d-48f7-9ebf-f0f724dbb2a9  2021-01-01 00:00:00.000000 UTC                          27                           1                                  1309                               10                         2021                             1                       male                            own                                   NaN                                     NaN                            car                         bad                            26-35                                    130.900000                                     1309.0                                      1309                                  10.0                                   10                                27                                 1                                NaN                                                 0.0                                            0.0\n",
      "3  46ad9e4b-1d0f-47b7-a73d-71cc66538b03  2021-01-01 00:00:00.000000 UTC                          23                           0                                 14555                                6                         2021                             1                       male                            own                                   NaN                                moderate                            car                         bad                            18-25                                   2425.833333                                    14555.0                                     14555                                   6.0                                    6                                23                                 0                                NaN                                                 0.0                                            0.0\n",
      "4  52ef7f25-6db2-47d0-9476-88bb8e6fa605  2021-01-01 00:00:00.000000 UTC                          22                           2                                  2462                               18                         2021                             1                       male                            own                                little                                  little            furniture/equipment                         bad                            18-25                                    136.777778                                     2462.0                                      2462                                  18.0                                   18                                22                                 2                                NaN                                                 0.0                                            0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the Feature Store clients used to fetch results\n",
    "from qwak.feature_store.offline import OfflineClientV2\n",
    "from qwak.feature_store.offline.feature_set_features import FeatureSetFeatures\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "FEATURE_SET = 'credit-risk-fs-pyspark'\n",
    "FEATURES_LIST = [ 'age', 'job', 'credit_amount', 'duration', 'year', 'month', 'sex', 'housing', 'saving_account',\n",
    "                'checking_account', 'purpose', 'risk', 'age_group', 'credit_duration_ratio', 'credit_amount_mean',\n",
    "                'credit_amount_sum', 'duration_mean', 'duration_sum', 'age_first', 'job_first', 'risk_first', 'credit_amount_diff_from_avg',\n",
    "                'duration_diff_from_avg']\n",
    "\n",
    "def fetch_training_features(start_time: datetime, end_time: datetime) -> pd.DataFrame: \n",
    "\n",
    "    offline_feature_store = OfflineClientV2()\n",
    "    \n",
    "    features = FeatureSetFeatures(\n",
    "        feature_set_name= FEATURE_SET,\n",
    "        feature_names= FEATURES_LIST)\n",
    "    \n",
    "    # It's recommended to be surrounded in a try/catch\n",
    "    features: pd.DataFrame = offline_feature_store.get_feature_range_values(\n",
    "        features=features,\n",
    "        start_date=start_time,\n",
    "        end_date=end_time\n",
    "    )\n",
    "\n",
    "    return features\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Define the date range for feature retrieval\n",
    "    feature_range_start = datetime(year=2023, month=1, day=1)\n",
    "    feature_range_end = datetime.today()\n",
    "\n",
    "    train_df = fetch_training_features(feature_range_start, feature_range_end)\n",
    "\n",
    "    print(f\"\\n\\nTraining data sample:\\n\\n{train_df.head().to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76ff96-59bf-4f5c-89d2-4450891cbeb5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Please note that although the Feature Set has been registered, it usually takes a couple of minutes to run the first ingestion job. This means you might not have any data to fetch until the ingestion job runs at least once.\n",
    "\n",
    "To verify the status of the ingestion, please refer to the JFrogML Dashboard -> Feature Sets -> `credit-risk-fs-pyspark` -> Executions.\n",
    "\n",
    "![Feature Store Dashboard](PNGs/ingestion-job-finished-pyspark.png)\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c4ec0-68f9-4657-91bb-31096e686acb",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Consuming Features for Real-Time Inference from the Online Store\n",
    "\n",
    "In the previous example, we retrieved historical data from the Offline Store, which is storing all the historical data. Now, we'll use the Online Store, which is optimized for real-time use-cases and provides a low-latency feature retrieval mechanism. \n",
    "JFrogML provides two ways to query the Online store and look up the most recent feature vector for a given key:\n",
    "\n",
    "###  1. Enriching Inference Requests with Features from Online Store\n",
    "\n",
    "JFrogML natively integrates the Model runtime with the Feature Store, offering an easy way to leverage very low-latency feature retrieval. This is done without specifically running a query, just by sending the feature set key in the model request input. This will automatically extract the latest features for that `key`, in our case `user` during a model serving request.\n",
    "\n",
    "\n",
    "Note: Below is an example code for local use only. If you're using it for a live model, please remove the `run_local` import.\n",
    "\n",
    "**The ModelSchema definition is mandatory to enable feature extraction via the OnlineClient or qwak.api decorator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9228cbeb-db89-4410-b021-b7cc2d654e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwak.model.tools import run_local # utility tooling for local testing and debugging - REMOVE BEFORE BUILDING REMOTELY\n",
    "\n",
    "from qwak.model.base import QwakModel\n",
    "from qwak.model.adapters import DefaultOutputAdapter, DataFrameInputAdapter\n",
    "from qwak.model.schema import ModelSchema, InferenceOutput\n",
    "from qwak.model.schema_entities import FeatureStoreInput, RequestInput\n",
    "import pandas as pd\n",
    "import qwak\n",
    "\n",
    "class CreditRiskModel(QwakModel):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def schema(self) -> ModelSchema:\n",
    "        model_schema = ModelSchema(\n",
    "            inputs= [FeatureStoreInput(name=f'{FEATURE_SET}.{feature}') for feature in FEATURES_LIST],\n",
    "            outputs=[InferenceOutput(name=\"credit_score\", type=float)]\n",
    "        )\n",
    "        return model_schema\n",
    "\n",
    "    @qwak.api(\n",
    "        feature_extraction=True,\n",
    "        input_adapter=DataFrameInputAdapter(),\n",
    "        output_adapter=DefaultOutputAdapter()\n",
    "    )\n",
    "    def predict(self, df: pd.DataFrame, extracted_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(f\"\\nInput dataframe df:\\n{df}\")\n",
    "        print(f\"\\nFeature Set extracted dataframe:\\n{extracted_df.to_string()}\")\n",
    "        return pd.DataFrame([['score', 0.5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedb207-6892-4f65-a61f-05ad7904b52e",
   "metadata": {},
   "source": [
    "Calling the model locally to test `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b15b602-fb16-4a28-aad6-161266975a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for: \n",
      "\n",
      " {\"user_id\":{\"0\":\"45b7836f-bf7c-4039-bc9e-d33982cc1fc5\"}}\n",
      "\n",
      "Input dataframe df:\n",
      "                                user_id\n",
      "0  45b7836f-bf7c-4039-bc9e-d33982cc1fc5\n",
      "\n",
      "Feature Set extracted dataframe:\n",
      "                                user_id  credit-risk-fs-pyspark.age  credit-risk-fs-pyspark.job  credit-risk-fs-pyspark.credit_amount  credit-risk-fs-pyspark.duration  credit-risk-fs-pyspark.year  credit-risk-fs-pyspark.month credit-risk-fs-pyspark.sex credit-risk-fs-pyspark.housing credit-risk-fs-pyspark.saving_account credit-risk-fs-pyspark.checking_account credit-risk-fs-pyspark.purpose credit-risk-fs-pyspark.risk credit-risk-fs-pyspark.age_group  credit-risk-fs-pyspark.credit_duration_ratio  credit-risk-fs-pyspark.credit_amount_mean  credit-risk-fs-pyspark.credit_amount_sum  credit-risk-fs-pyspark.duration_mean  credit-risk-fs-pyspark.duration_sum  credit-risk-fs-pyspark.age_first  credit-risk-fs-pyspark.job_first credit-risk-fs-pyspark.risk_first  credit-risk-fs-pyspark.credit_amount_diff_from_avg  credit-risk-fs-pyspark.duration_diff_from_avg\n",
      "0  45b7836f-bf7c-4039-bc9e-d33982cc1fc5                          27                           2                                  4576                               45                         2023                             3                       male                            own                              moderate                                moderate                            car                        good                            26-35                                    101.688889                                       4576                                      9152                                    45                                   90                                27                                 2                              null                                                   0                                              0\n",
      "\n",
      "Prediction:  [{\"0\":\"score\",\"1\":0.5}]\n"
     ]
    }
   ],
   "source": [
    "def test_model_locally():\n",
    "    # Create a new instance of the model\n",
    "    m = CreditRiskModel()\n",
    "\n",
    "    # Define the columns\n",
    "    columns = [\"user_id\"]\n",
    "\n",
    "    # Define the data\n",
    "    data = [[\"45b7836f-bf7c-4039-bc9e-d33982cc1fc5\"]]\n",
    "    \n",
    "    # Create the DataFrame and convert it to JSON\n",
    "    json_payload = pd.DataFrame(data=data, columns=columns).to_json()\n",
    "\n",
    "    print(\"Predicting for: \\n\\n\", json_payload)\n",
    "    \n",
    "\n",
    "    # Run local inference using the model and print the prediction\n",
    "    # The run_local function is part of the qwak library and allows for local testing of the model\n",
    "    prediction = run_local(m, json_payload)\n",
    "    print(\"\\nPrediction: \", prediction)\n",
    "\n",
    "test_model_locally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed62ec8-3f5f-4be5-976c-597ed647a2cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "As you can see, the we only sent the `user` ID in the prediction request, and JFrogML automatically extracted the relevant (latest) features for that key from the Feature Set specified in the Model Schema. \n",
    "\n",
    "This approach is automatically logging the extraction latency to the model Analytics.\n",
    "\n",
    "<br>\n",
    "\n",
    "###  2. Features Lookup with the OnlineClient\n",
    "\n",
    "With the previous approach we managed to enable a JFrogML model object to fetch features automatically and that approach is great for most cases. However what happens if we want to have more control over the keys we want to look up for at runtime, like for example looking up multiple keys for a single prediction request input. \n",
    "\n",
    "That's what the `OnlineClient` is for, to enable you explicit queries, as we'll exemplify below:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90cf537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Realtime features extracted:\n",
      "\n",
      "                                user_id  credit-risk-fs-pyspark.age  credit-risk-fs-pyspark.job  credit-risk-fs-pyspark.credit_amount  credit-risk-fs-pyspark.duration  credit-risk-fs-pyspark.year  credit-risk-fs-pyspark.month credit-risk-fs-pyspark.sex credit-risk-fs-pyspark.housing credit-risk-fs-pyspark.saving_account credit-risk-fs-pyspark.checking_account credit-risk-fs-pyspark.purpose credit-risk-fs-pyspark.risk credit-risk-fs-pyspark.age_group  credit-risk-fs-pyspark.credit_duration_ratio  credit-risk-fs-pyspark.credit_amount_mean  credit-risk-fs-pyspark.credit_amount_sum  credit-risk-fs-pyspark.duration_mean  credit-risk-fs-pyspark.duration_sum  credit-risk-fs-pyspark.age_first  credit-risk-fs-pyspark.job_first credit-risk-fs-pyspark.risk_first  credit-risk-fs-pyspark.credit_amount_diff_from_avg  credit-risk-fs-pyspark.duration_diff_from_avg\n",
      "0  06cc255a-aa07-4ec9-ac69-b896ccf05322                          31                           2                                  1935                               24                         2021                             1                       male                            own                                little                                moderate                       business                         bad                            26-35                                     80.625000                                       1935                                      1935                                    24                                   24                                31                                 2                              null                                                   0                                              0\n",
      "1  46ad9e4b-1d0f-47b7-a73d-71cc66538b03                          23                           0                                 14555                                6                         2021                             1                       male                            own                                  null                                moderate                            car                         bad                            18-25                                   2425.833333                                      14555                                     14555                                     6                                    6                                23                                 0                              null                                                   0                                              0\n",
      "2  95ec0c53-4e27-4490-b85f-1448de70fc26                          25                           1                                   685                               12                         2021                             1                       male                            own                                little                                moderate                            car                         bad                            26-35                                     57.083333                                        685                                       685                                    12                                   12                                25                                 1                              null                                                   0                                              0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from qwak.feature_store.online.client import OnlineClient\n",
    "from qwak.model.schema_entities import FeatureStoreInput\n",
    "from qwak.model.schema import ModelSchema\n",
    "\n",
    "model_schema = ModelSchema(\n",
    "    inputs= [FeatureStoreInput(name=f'{FEATURE_SET}.{feature}') for feature in FEATURES_LIST],\n",
    "    outputs=[InferenceOutput(name=\"credit_score\", type=float)]\n",
    ")\n",
    "    \n",
    "online_client = OnlineClient()\n",
    "\n",
    "df = pd.DataFrame(columns=['user_id',],\n",
    "                  data   =[['06cc255a-aa07-4ec9-ac69-b896ccf05322'],\n",
    "                           ['46ad9e4b-1d0f-47b7-a73d-71cc66538b03'],\n",
    "                           ['95ec0c53-4e27-4490-b85f-1448de70fc26']])\n",
    "                  \n",
    "online_features = online_client.get_feature_values(model_schema, df)\n",
    "\n",
    "\n",
    "print(f\"\\nRealtime features extracted:\\n\\n{online_features.to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4cae4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You may have noticed that the FeatureStoreInput names contain both the feature set name and the feature name. This design allows you to specify and utilize multiple feature sets within the same request.\n",
    "\n",
    "Similar to the previous option, the `ModelSchema` is a required component. It informs JFrogML about the features to include in the lookup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
